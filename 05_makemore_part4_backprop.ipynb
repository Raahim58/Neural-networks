{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iuv3Lkt7g-w2",
        "toGU79cyl8HI",
        "ygSL4qssnpCq",
        "gpa3eeHNYKrR",
        "iRDhMDgO9Boa"
      ],
      "authorship_tag": "ABX9TyOTq1XV41iQ9q1EiBQH+A/e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raahim58/Neural-networks/blob/main/05_makemore_part4_backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05. Building Makemore from scratch (Part4: Becoming a backprop ninja)\n",
        "\n",
        "* Makemore makes more of the things you give it.\n",
        "* makemore takes one text file as input, where each line is assumed to be one training thing, and generates more things like it. Under the hood, it is an autoregressive character-level language model, with a wide choice of models from bigrams all the way to a Transformer (exactly as seen in GPT). For example, we can feed it a database of names, and makemore will generate cool baby name ideas that all sound name-like, but are not already existing names. Or if we feed it a database of company names then we can generate new ideas for a name of a company. Or we can just feed it valid scrabble words and generate english-like babble.\n",
        "* it is basically a bigram character-level language model\n",
        "\n",
        "**Resources:**\n",
        "\n",
        "* makemore github repo: https://github.com/karpathy/makemore/tree/master\n",
        "\n",
        "* tutorial lecture 5 (makemore part 4) code: https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part4_backprop.ipynb\n",
        "\n",
        "* MLP model based on 2003 paper: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
        "\n",
        "* makemore pre defined code: https://github.com/karpathy/makemore/blob/master/makemore.py\n",
        "\n",
        "* link to youtube lecture 4: https://www.youtube.com/watch?v=q8SA3rM6ckI&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=5\n",
        "\n",
        "* whole lecture series code: https://github.com/karpathy/nn-zero-to-hero\n",
        "\n"
      ],
      "metadata": {
        "id": "tGjfymQ4gfN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Getting data + intro\n",
        "\n",
        "* we are going to be doing backpropogation manuall instead of using `loss.backward()`. It is a very important exercises because:\n",
        "  * it is a leaky abstraction: backprop doesn't make your network magically so you need to understand it under the hood if you need to debug it and make it run in your neural net\n",
        "  * it will make everything explicit so you know what's going on\n",
        "* we've covered autograd and we wrote micrograd. Micrograd was an autograd engine only on the level of individual scalars so the atoms were individual single numbers\n",
        "* running backprop manually right now is not recommended because it's very time consuming as compared to `loss.backward()`\n",
        "* we'll lose the `loss.backward()` and do the backprop manually\n",
        "\n"
      ],
      "metadata": {
        "id": "iuv3Lkt7g-w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "NQpgJv0WhFY0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/karpathy/makemore/raw/master/names.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXRZN9iLhFBJ",
        "outputId": "0844d975-919d-476b-f78d-cb81a1d5375c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-28 06:23:11--  https://github.com/karpathy/makemore/raw/master/names.txt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/karpathy/makemore/master/names.txt [following]\n",
            "--2024-08-28 06:23:11--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "names.txt           100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-08-28 06:23:12 (6.26 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MrqPT1_hNwx",
        "outputId": "9a9b048d-4914-4e01-9c0a-6546cf0f6922"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsGYUUUokR8s",
        "outputId": "162c3bf6-e19a-467b-8c9b-5b291a0cf37e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_Hv2IlCkTnz",
        "outputId": "dc37a994-c7d4-4185-d10f-36fdff9d80a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item() # value exactly equal?\n",
        "  app = torch.allclose(dt, t.grad) # value approximately equal due to fpn arithmetic?\n",
        "  maxdiff = (dt - t.grad).abs().max().item() # abs max value difference\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "id": "f30SG2YMkWhO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* we're going to have gradients that we estimate manually ourselves, and we're going to have gradients that PyTorch calculates\n",
        "* we're going to be checking for correctness assuming that PyTorch is correct"
      ],
      "metadata": {
        "id": "LfMB5jyfky-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5av19w7zkpYo",
        "outputId": "f7b87254-6810-4b7d-83bb-59c807c57f6a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* the initialization has been changed a little bit to be small numbers. So normally we would set the biases to be 0, here we are setting them to small random numbers. We do this because if our variables are initialized exactly 0, sometimes it can mask an incorrect implementation of a gradient because when everything is 0, it simplifies and sort of gives a simpler expression of the gradient\n",
        "* we use b1 anyways despite of having a batchNorm layer later which will just cancel out the bias because we'll have a gradient w.r.t it and we can check if we're still calculating it correctly"
      ],
      "metadata": {
        "id": "AGNhK8djlNbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating a single batch\n",
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ],
      "metadata": {
        "id": "4GyTb9Klkq1N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gohNYPLRktfh",
        "outputId": "2df1e90f-caa1-4dce-a8ec-7469fb1c29bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3401, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* forward pass is quite expanded from the previous lecture because:\n",
        "  1. normally we used `cross_entropy` but over here we usr an explicit manual implementation of the loss function\n",
        "  2. we broke the implementation in small chunks so we have a lot of intermediary tensors along the way, and this is because we're about to go backwards and calculate the gradient in this backprop from the bottom too the top\n",
        "    * in forward pass we have log probs tensor, but in backward pass we'll have a delog probs which will store the derivative of the loss function w.r.t the loss log function\n",
        "* we tell PyTorch that we retain the grad's of every value because in exercise 1 we're going to calculate the backward pass -> we'll calculate all these `de` variables and then use the cmp function to check our correctness according to what PyTorch is telling us\n",
        "* in exercise 2 we break up the loss and backpropogate through it manually in all the atomic pieces that make it up but here we will collapse the loss in a single `cross_entropy` call and instead we're going to analytically derive using mth and paper and pencil, the gradient of the loss w.r.t logits. Instead of backproping through all of its little chunks one at a time, we will just going to analytically derive what that gradient is and we're going to implement that\n",
        "* we'll do the same with batch normalization. instead of breaking batchNorm in little pieces, we're going to use maths to derive the gradient through the batchNorm layer -> much more efficient instead of backpropogating through its layers independently: exercise 3"
      ],
      "metadata": {
        "id": "bcQvh639mBOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Exercise 1: backpropogating through the atomic compute graph\n",
        "\n"
      ],
      "metadata": {
        "id": "toGU79cyl8HI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Backpropogating through `loss`"
      ],
      "metadata": {
        "id": "uFsGfALO45KB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logprobs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dzUo6gTDwew",
        "outputId": "116858f7-9c2c-40d8-9887-f65644797e7c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `dlogprobs`: we need to understand what will go here to calculate the gradient of the loss w.r.t all the elements of the logprobs tensor\n",
        "  * will hold the derivative of the loss w.r.t all the elements of `logprobs` -> `dlogprobs` will be the same shape/size as `logprobs`\n",
        "  * how does it influence loss?\n",
        "    * loss is negative `logprobs` indexed with a range of n and Yb with the mean of that (Yb is an array of all the correct indices of the next character in the sequence)\n",
        "    ```\n",
        "    loss = -logprobs[range(n), Yb].mean()\n",
        "    ```\n",
        "    * once we've plucked out the examples, we're taking the mean and then the negative so essentially (n = size of the batch):\n",
        "    ```\n",
        "    loss = -(a + b + c) / 3\n",
        "    dloss/da = 1/3 -> -1/n\n",
        "    ```\n",
        "    * only 32 probabilites from `logprobs` participate in the loss calculation so what's the derivative of all the other most elements that do not get plucked out, well their gradient is intuitivel is 0 since they don't participate in the loss so most of these numbers inside this tensor does not feed into the loss so if we were to change these numbers, the loss doesn't change which is the equivalent way of saying that the derivative of the loss w.r.t them is 0 (they don't impact it)\n",
        "\n"
      ],
      "metadata": {
        "id": "K4MkJJW5J_LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `logprobs` depends on log:\n",
        "```\n",
        "logprobs = probs.log()\n",
        "```\n",
        "  * all elements of probs are element wise being applied log to\n",
        "  * in micrograd there is a `log` node which takes in `probs` and creates `logprobs`; `dprobs` will be the local derivative of that individual operation `log` * derivative loss w.r.t its output (in this case `dlogprobs`)\n",
        "    * local derivative of this operation (taking `log` element wise) -> d/dx (logx) = x -> 1/probs\n",
        "    * then use chain rule to get dprobs and compare the values\n",
        "    * probs is going to be inverted and then element wise be applied to dlogprobs so if `probs` is very close to 1 means your network is predicting the character quickly and `dprobs` will be 1/1 with `dlogprobs` just passing through. If probs is incorrectly assigned (low probs) then dprobs will be huge\n",
        "    * basically what is happening is intuitively it is taking the examples that have a very low probability assigned and is boosting their gradient\n",
        "\n"
      ],
      "metadata": {
        "id": "Fen8BI01KBh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `dcounts_sum_inv`: first you take the `logits` coming out of the neural net and then find the max in each row, and then subtract it for the purposes of numerical stability\n",
        "  * if you don't do this you run into numerical issues if some of the logits take on too large values if we end up exponentiating them -> essentially for safety\n",
        "  * `counts` are then created by exponentiating the `logits` and then we take the sum of these counts and normalize so the probs sum to 1 -> then invert them to get `counts_sum_inv`\n",
        "  * in this multiplication:\n",
        "  ```\n",
        "  probs = counts * counts_sim_inv\n",
        "  ```\n",
        "  here we have an implicit broadcasting that PyTorch will do because it needs to take this column tensor of 32 numbers and replicate it horizontally 27 times to align these 2 tensors and do an element wise multiply, using a toy example:\n",
        "  ```\n",
        "  c = a * b # but with tensors:\n",
        "  a[3x3] * b[3,1]\n",
        "  a11*b1 a12*b1 a13*b1\n",
        "  a21*b2 a22*b2 a23*b2\n",
        "  a31*b3 a32*b3 a33*b3\n",
        "  c[3x3]\n",
        "  ```\n",
        "    * this looks like a single operation but it is actually 2 operations applied sequentially:\n",
        "      1. replication: it took the column tensor and replicated across all the  columns (27 times)\n",
        "      2. multiplication: let's backprob through the element wise multiplication: local derivative (counts) * dprobs (due to the chain rules)\n",
        "        * this is the derivative or gradient w.r.t replicated `b`. We don't have a replicated `b` we just have a single `b` column, how do we back propogate through the replication? -> `b1` is similar to `b2` and `b3` but is just being used multiple times\n",
        "        * to backprop through the replication, we need to summ all the gradients that arrive at any one node (if a node is used multiple times, the gradient for all of its uses sum during back prop). Since `b1` is used multiple times, the right thing to do is to sum horizontally across all the rows with dim=1 but we want to retain dimension so that the `counts` and the its gradient are going to exactly be the same shape so we keep `keepdim=True` so that we don't lose this dimension and this will make `dcounts_sum_inv.shape` be exactly [32, 1] -> then compare it\n",
        "\n"
      ],
      "metadata": {
        "id": "m5l7ZAdFKDCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts.shape, counts_sum_inv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnlEF_eGQ80C",
        "outputId": "86138fb5-12d9-4ccd-a781-237fe5927bfa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]), torch.Size([32, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `dcounts` utilizes `counts_sum_inv` which utilizes `counts` so `counts` will actually be used twice to backpropogate (will be done ahead for the 2nd time)\n",
        "\n",
        "* `dcounts_sum` will be found by the local derivative of `counts_sum` * chain rule (`dcounts_sum_inv`)\n",
        "\n"
      ],
      "metadata": {
        "id": "XgBebW8XKF1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* backpropogating through `counts_sum` which is counts.sum along the rows, we have to make sure the `count.shape` = [32, 27] and `counts_sum.shape` = [32, 1], so in backprop we need to take the column of derivatives and transform it into a array of 2D derivative\n",
        "  * basically we take some kind of input like a 3x3 matrix `a` and we sum up the rows into a column tensor `b1` -> we've got the derivative w.r.t to all `b's` now we want the local derivative w.r.t to `a's` -> how do `a's` depend on `b's` so what is the local derivative of this operation:\n",
        "  ```\n",
        "  a11 a12 a13 ---> b1 ( = a11 + a12 + a13)\n",
        "  a21 a22 a23 ---> b2 ( = a21 + a22 + a23)\n",
        "  a31 a32 a33 ---> b3 ( = a31 + a32 + a33)\n",
        "  ```\n",
        "    * `b1` here depends on the elements in the first row, hence derivative w.r.t them is 1, and it's derivative w.r.t to all the elements in the rows down below is 0\n",
        "    * local derivative is just 1 so we land with the derivative of `b1` only, we can look at it as an addition which is a router of a gradient -> whatever gradient comes from above, it just gets routed equally to all the elements that participate in the addition so in this case the derivative of `b1` will flow equally to the derivative of `a11` `a12` `a13` horizontally\n",
        "    * what we want to do is take `dcounts_sum` with size [32, 1] and we just want to replicate it 27 times horizontally to create a [32, 27] array -> so this way we're letting the broadcasting implement the replication\n",
        "    * we calculate `dcounts` twice so we need to add the 2 results of the 2 branches\n",
        "\n"
      ],
      "metadata": {
        "id": "BZI01NEcKHIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `dnorm_logits`: `counts` is an element wise operation so everything is very simple with `norm_logits.exp()` with its local derivative being the same and the chain rule being `dcounts`\n",
        "\n",
        "* we are backproping through this line:\n",
        "```\n",
        "norm_logits = logits -  logit_maxes\n",
        "```\n",
        "  * we've got `dnorm_logits` and we need `dlogits` and `dlogit_maxes`\n",
        "  * we need to be careful here because the shapes here are not the same so there's an implicit broadcasting happening.\n",
        "  * we have this happening:\n",
        "  ```\n",
        "  c32 = a32 - b3\n",
        "  ```\n",
        "    * for every element c we have to look at how it came to be -> every element `c` is that element of `a` minus that associated value of `b`\n",
        "    * the derivatives for the `c`'s will flow equally to the corresponding `a`'s and also to the corresponding `b`'s, and in addtional the `b`'s are broadcast so we'll have to do an additional sum just like we did before\n",
        "    * `dlogits` will exactly copy the derivative on `norm_logits`\n",
        "    * `logit_maxes` is a column -> as we saw before because we keep replicating the same elements across all the columns, then in the backward pass because we keep reusing this, these are all just like seperate branches of use of that one variable and therefore we have to do a sum along `dim=1` with `keepdims=True` so that we don't destroy the dimension, and `logit_maxes` will be the same shape\n",
        "    * we have to be careful because this `dlogits` is not the final `dlogits` and that's because not only do we get gradient signal into logits through `norm_logits`, but `logit_maxes` as a function of `logits` as well, and that's a second branch into `logits` so not our final derivative (will come back to it later for the second branch)\n",
        "\n"
      ],
      "metadata": {
        "id": "nsFlV1j-KI3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* we're doing this for the numerical stability of the `softmax` we're implementing. If we take one row of these logits and add or subtract any value equally to all the elements then the value of the `probs` will be unchanged. The `norm_logits = logits - logit_maxes` make sure that `norm_logits.exp()` doesn't overflow, and the reason we're using `logit_maxes` is then we're guaranteed that each row of logits, the highest number, is 0, hence this will be safe.\n",
        "  * that has repercussions. If it is the case that changing `logit_maxes` does not change the `probs` and therefore does not change the loss, then the gradient on the `logit_maxes` should be 0. Due to floating point, we get very small numbers for most but not exactly 0. So, this is telling us that the values of `logit_maxes` are not impacting the final loss as they should.\n",
        "  * If you don't back propogate piece by piece, you skip the branch and assume its gradient is 0\n",
        "\n"
      ],
      "metadata": {
        "id": "hp1KS6UrKMvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* we just backproped through `logit_maxes` and now let's do so through `logits` from the `logit_maxes = logits.max(1, keepdimg=True).values` through the second branch.\n",
        "  * the max returns both the value, and it returns the indices at which those values occur to count the max value\n",
        "  * in the forward pass we only used the values but in the backward pass it's extremely useful to know where those maximum values occured and thus we have the indices\n",
        "  *  we have the logits tensor which is [32, 27] and in each row we find a max value, and that value gets plucked out into `logit_maxes` so intuitively the derivative flowing through here then should be: **1 * (local derivative of the appropiate entry that was plucked out) * (global derivative of `logit_maxes`)** -> so really what we're doing here is we're taking the `dlogit_maxes` and we need to scatter it to the correct positions in these `logits` from where the max values came\n",
        "  * we could do it kind of similar to what we did with `logprobs` where we created a zeros and then we populated the correct elements using the correct indices and then set them to 1.\n",
        "  * We could also do it using one-hot. So `f.one_hot` and then we take the `logits` of max over the first dimension (`.indices`). So i'm telling pytorch that the dimension of each of these tensors should be 27. What this will do is create an array of where the maxes came from in each row and that element is 1, and the rest of the elements are zero. So it's a one-hot vector in each row and these indices are now populating a single one in the proper place.\n",
        "    * we then multiply with `dlogit_maxes` which is a columnn of [32, 1]. Hence `dlogit_maxes` will broadcast and that column will get replicated and in an element wise multiply will ensure that each of these just gets routed to whicever one of these bits is turned on\n",
        "    * we use addition since this is the second branch of `dlogits`\n",
        "\n"
      ],
      "metadata": {
        "id": "WyqB5nwqKOg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Yb, logprobs[range(n), Yb]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXvBHZ0DEo1f",
        "outputId": "806cc887-f664-4e71-fab7-21c3bdd1d811"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n",
              "         26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18]),\n",
              " tensor([-4.0244, -2.9800, -3.6180, -3.3766, -4.0983, -3.5030, -3.1907, -4.1117,\n",
              "         -3.2165, -4.1739, -3.1300, -1.5701, -2.8552, -2.9549, -2.9740, -3.1963,\n",
              "         -3.9360, -3.1111, -3.5279, -3.3466, -2.8598, -2.9436, -4.3067, -3.9901,\n",
              "         -3.5353, -2.8794, -2.9267, -3.8067, -2.7514, -3.5290, -3.3112, -3.1495],\n",
              "        grad_fn=<IndexBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Backpropogating through linear layer 2"
      ],
      "metadata": {
        "id": "LBCDsRcX5IyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm_logits.shape, logits.shape, logit_maxes.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf_vpLQs8JHm",
        "outputId": "dee256fb-44fb-4857-bfa9-efdc6061afb9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]), torch.Size([32, 27]), torch.Size([32, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **backward pass for linear layer:**\n",
        "we'll continue with `logits` which is an outcome of a matrix multiplication and a bias offset in this linear layer:\n",
        "```\n",
        "logits = h @ W2 + b2 # output layer\n",
        "```\n",
        "  * we have a 64D hidden states. Then the W matrix projects those 64D vectors into 27D and then there's a 27 dimensional offset which is a 1D vector.\n",
        "  * the plus actually broadcasts because `h` * `W2` will give us a [32, 27] so the `+b2` is a 27D vector. In the rules of broadcasting, what will happen to bias vector is that this 1D vector of 27 will get aligned with a padded dimension of one on the left and it will basically become a row vector and then it will get replicated vertically 32 times to make it [32, 27] and then there's an element-wise multiply\n",
        "  * pick a small example and evaluate it to understand the pattern and then generalize and write out the general formula for how these derivatives flow in an expression like :\n",
        "  ```\n",
        "  d = a @ b + c\n",
        "  d11 = a11b11 + a12b21 + c1\n",
        "  d12 = a11b12 + a12b22 + c2\n",
        "  d21 = a21b11 + a22b21 + c1\n",
        "  d22 = a21b12 + a22b22 + c2\n",
        "  ```\n",
        "    * we have the derivative of the loss w.r.t to `d` and we'd like to know the derivative of the loss w.r.t `a` `b` and `c`\n",
        "    * bias vector becomes a row vector in the broadcasting, and will replicate vertically **(look at vectors pic or at 47:12)**. Turns out all of the formulas we derived here by taking gradients can actually be expressed as a matrix multiplication -> the `b` matrix has transposed. Long story short, `dL/da is simply equal to dL/dt @ b^T(transpose)`\n",
        "    * w.r.t `b`: dL/db is also a matrix multiplicatin and in this case you have to take the matrix `a` and transpose it and matrix multiply it by dL/dd giving as `dL/db`\n",
        "    * w.r.t `c`: dL/Dc since you're just offsetting these expressions, you just have to take the dL/dd matrix of the derivatives of `d` and you just have to sum across the columns\n",
        "    * overall, backward pass of a matrix multiply is matrix multiply. Instead of just like we had `d = a @ b + c` in the scalar case, we arrived at something very similar using matrix mul. instead of a scalar mul.\n",
        "    ```python\n",
        "    dL/da = dL/dd @ b^T\n",
        "    dL/db = dL/dd @ a^T\n",
        "    dL/dc = dL/dd @ sum(0)\n",
        "    ```\n",
        "  * so when we're creating `dh` we know it needs to have the same shape as `h`. `dh` should be some kind of matrix multiplication of the `logits` with `W2` and `dlogits` is [32, 27] and `W2` is [64, 27]. There is only a single way to make the shape workout and that is through `dlogits @ W2.T` -> transpose `W2` to make the shapes workout\n",
        "  * `dW2` is a matrix mul. of `dlogits` and `h` and there is one transpose there too. `W2.shape` is [64, 27] so i need to transpose `h` (becomes [64, 27]) to matrix multiply it with [32, 27] giving us a [64, 27] so we multply it with `dlogits` as that's the only method.\n",
        "  * `db2` is a vertical sum along the 0th axis because `b2.shape` is 27 so in order to get `dlogits` which here is [32, 27] so knowing its `dlogits` over `sum` in some direction, that direction must be 0 since we need to eliminate the dimension.\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/Raahim58/Neural-networks/main/images/vectors.png\" height=300 width=400 alt=\"vector 2\"/>\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/Raahim58/Neural-networks/main/images/vectors2.png\" height=300 width=400 alt=\"vector 2\"/>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zWNMyw2XKQK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Backpropogating through non-linear layer"
      ],
      "metadata": {
        "id": "oSEkv16r5RLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* we have the derivative for `h` as `dh` already so we need to backprop through `tanh` into `hpreact`:\n",
        "```python\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "```\n",
        "  * so we want to derive `dhpreact` and here we have to backprop through `tanh` which we already did in micrograd\n",
        "    * if we have `a = tanh(z)` then `da/dz = 1 - a^2` as the local derivative, where `a` is the output of the `tanh` not the input of the `tanh`. For us `a` is `h`, with the chain rule being `dh`."
      ],
      "metadata": {
        "id": "3GCE4tqSRdP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Backpropogating through BatchNorm layer"
      ],
      "metadata": {
        "id": "LleMIHs85YJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* we have `dhpreact` and need `dbngain` `dbnraw` and `dbnbias`:\n",
        "```python\n",
        "hpreact = bngain * bnraw + bnbias # * different from matrix multiply @\n",
        "```\n",
        "  * @ is a dot products between rows and columns for these matrices involved meanwhile * is an element wise multiply so things are simpler\n",
        "  * we do have to be careful with the broadcasting happening here. `bngain` and `bnbias` are [1, 64] but `hpreact` and `bnraw` are [32, 64] so we have to makre sure that all the shapes work out fine\n",
        "  * `dnbngain`, whenever we have `a*b + c`, we saw the local derivative here is just the other element with chain rule as `dhpreact`. We have to be careful because `bngain` is [1, 64] meanwhile `dhbngain` is [32, 64] and so the correct thing to do is sum as `bngain` is being replicated, and therefore all the gradients in all of the rows that are now flowing backwards need to sum up to that same tensor `dbngain`. We keep the dimensions true so we that the tensor shapes work.\n",
        "  * `dbnraw` will just be the other element which is `bngain` * `dhpreact` (chain rule). For the right dimensions we need `dhpreact` as [32, 64], `bngain` is [1, 64] so it will just get replicated to create this multiplication (gets replicated same way in forward pass)\n",
        "  * `dbnbias` is very similar to the bias we saw in the linear layer as `b2`. The gradients from `hpreact` will just flow into the biases and add up as these are just offsets. The 0th dimension is the examples so we sum along that dimension the same way the bias gets replicated vertically giving us [1, 64].\n",
        "  \n"
      ],
      "metadata": {
        "id": "AtiqDBs6Ve8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "batchNorm layer:\n",
        "* `bngain` and `bnbias` are the parameters so the backprop ends but `bnraw` is the output of the standardisation so here we break the batchNorm layer so that we can backprop through each line individually. What's happening is that:\n",
        "  * `bnmeani` is the sum,\n",
        "  * `bndiff = x - mean`,\n",
        "  * `bndiff2 = x - mean^2`, `bnvar` is the variance so sigma^2 which is basically the sum of squares so `bndiff2.sum(0)` -> we normalize by 1/n-1 instead of 1/n (explained later as bessel's correction).\n",
        "  * `bnvar_inv` then becomes `bnvar + 1e-5 (epsilon)` with being 1/sqrt(expression)\n",
        "  * `bnraw` which is the xhat here is equal to the `bndiff` (the numerator) * `bnvar_inv`\n",
        "    * for `dbnraw` we need to backprop through its line. Through writing out the shapes we get to know that `bnvar_inv` is a shape [1. 64] so there is a broadcasting happening here which we need to be careful with. It's just an element wise mul. so do the same (choose the other element `bnvarinv` as the local derivative and then use chain rule to get the output `bnraw` here).\n",
        "    * Same for `dbnvar_inv`. It will take [32, 64] and multiplying it by [32, 64] but `bnvar_inv` is [1, 64] hence for `dbnvar_inv` we need a sum across the dimension 0 with `keepdims=True`. `bndiff` has 2 branches so we'll do its second branch as we go on forward"
      ],
      "metadata": {
        "id": "2DxobgEGZoto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* backproping through:\n",
        "```python\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5 # 1e-5 = epsilon\n",
        "```\n",
        "  * we use the power rule where now `dbnvar = -0.5 * x(which is = bnvar+ 1e-5)**-1.5`. We also need to do a chain rule because we need to take further the derivative of `bnvar` w.r.t expression as `x`, but since this is an element wise operation and everything is fairly simple, that's just 1. The expression uptil now is the localderivative so we multiply it by the global derivative which is `dbnvar_inv`."
      ],
      "metadata": {
        "id": "kq1rx33kc_N3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bissel's correction in `bnvar`**\n",
        "\n",
        "* there are two ways of estimating variance of an array, one is the biased estimate which is 1/n and the other one is an unbiased estimate which is 1/(n-1) -> not mentioned clearly in the paper\n",
        "  * the paper uses the biased estimated (1/n) at training time but when they do inference they used unbiased estimate (1/(n-1)). Basically they introduce a train test match where in training they use the biased version and in test time they use the unbiased version\n",
        "  * unbiased estimate or 1/(n-1) gives a better estimate of the variance in case where one has samples for the population that are very small and that is the case for us since we're dealing with mini batches, and these mini batches are a small samples of a larger population which is the entire training set. So it turns out that if one just estimate using 1/n, it almost always underestimates the variance\n",
        "  * we should not really have train test discrepancies. PyTorch does take in an argument through batchNorkm to ask whether one wants to use the unbiased version or the biased in both train and test. This discrepancy can often decrease when the batch sizes go larger but still better to stay away from this \"bug\"."
      ],
      "metadata": {
        "id": "o5-IX2Jpn7n9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "backproping through `bnvar`:\n",
        "* First we scrutinize the shapes with `bnvar.shape` as [1, 64] and `bndiff2.shape` as [32, 64]. We do the sum over the 0th axis to squash the first dimension. This statement of a sum across the 0th dimension from the forward pass indicates that there will be some sort of replication or broadcasting in the backward pass along the same dimension. On the other hand, a broadcasting or replication in the forward pass indicates a variable reuse in the backward pass turning into a sum over the exact same dimension.\n",
        "* so we have replication in the backward pass. So we have a 2D array at the end of `bndiff2` which we are scaling by a constant (1/n-1) and then we sum over the columns vertically and scale to get a row vector `b1` and `b2`:\n",
        "\n",
        "```python\n",
        "a11 a12\n",
        "a21 a22\n",
        "--->\n",
        "b1 b2 where:\n",
        "b1 = 1/(n-1)*(a11 + a21)\n",
        "b2 = 1/(n-1)*(a12 + a22)\n",
        "```\n",
        "  * basically the derivative of `b1` has to flow through the columns of `a` scaled by 1/(n-1)\n",
        "  * intuitively the derivative flow tells us that `dbndiff2` will be the local derivative of this operation:\n",
        "    * create a large array of 2D one's\n",
        "    * scale it by 1.0/(n-1) -> sort of the local derivative: [32, 64]\n",
        "    * for the chain rule we will just multiply by `dbnvar` -> [1, 64]\n",
        "    * we let the broadcasting do the replication because in pytorch `dbnvar` will in this multiplication get copied vertically until the two are of the same shape and then there will be an element wise multiply"
      ],
      "metadata": {
        "id": "iYqZmvxOqrJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "differentiating `bndiff2` to get `dbndiff`:\n",
        "  * simple operation since to get `bndiff2`, `bndiff` is just being square which is an element wise operation in a scalar case\n",
        "  * hence squared differentation is just 2x so it becomes `2*bndiff * dbndiff2 (chain rule)`\n",
        "  * we already called `dbndiff` once so we need to add it now to the previous branch\n"
      ],
      "metadata": {
        "id": "gRyQVlFTznDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "backprogating through `bndiff` to get `dhprebn` and `dbnmeani`:\n",
        "* `bndiff.shape` = [32, 64], `hprebn.shape` = [32, 64] and `bnmeani.shape` = [1, 64]\n",
        "* `bndiff = hprebn - bnmeani` where the minus sign will broadcast for us -> means variable reuse in the backward pass in the form of a sum\n",
        "  * since all of the variables in the equation are of the same shape then the local derivative for each of the elements here is just one. What this means is that gradient just simply copies so it's just a variable assignment, it's a quality.\n",
        "  * we clone it to just be safe to create an exact copy of `dbndiff`\n",
        "  * `dbnmeani`: local derivative: `-torch.ones_like(bndiff)` * chain rule differential (`dbndiff`) -> backpropogation for the replicated `bnmeani` so we still have to backprop through the replicated and broadcasted so we do that by doing a sum over the 0th dimension which was a replication\n",
        "    * if we scrutinize this we realize that `-torch.ones_like(bndiff)` is the same shape as `dbndiff` so no point in multiplying one's so we just make it `-dbndiff.sum(0)`\n",
        "  * we should get `dhprebn` wrong because we are backpropogating from `bndiff1` into `hprebn` but we're not done because `bnmeani` depends on `hprebn` and this will be a second portion of the derivative coming from the second branch"
      ],
      "metadata": {
        "id": "LOflcUOu0bQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "backpropogating from `bnmeani` into `hprebn`:\n",
        "* again there is a sum in the 0th dimension so there will be broadcasting in the backward pass\n",
        "* `dhprebn` -> gradient, which is `dbnmeani` will be scaled by 1/n, and then it's going to flow across all the columns and deposit itself into the `hprebn`.\n",
        "* After this we need to replicate using `torch.ones_like()` of `hprebn` over all the rows -> broadcasting\n",
        "* we add this to the previous branch"
      ],
      "metadata": {
        "id": "ZCtr_HA43JNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Backpropogating through linear layer 1:\n",
        "\n"
      ],
      "metadata": {
        "id": "J9syjFEr4lDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "hprebn = embcat @ W1 + b1 # forward pass\n",
        "```\n",
        "* inspecting the shapes first:\n",
        "  * hprebn.shape = [32, 64]\n",
        "  * embcat.shape = [32, 30]\n",
        "  * W1.shape = [30, 64]\n",
        "  * b1.shape = [64] # 1D\n",
        "* we just need to match the shapes:\n",
        "  * `dembcat` should be some matrix multiplicating of `dhprebn` with `w1`  and one transpose thrown in there. To make `embcat` be [32, 30], we need to take `dhprebn` and multiply it by W1^T (transpose)\n",
        "  * to get `dW1` we need to end up with [32, 64] so to get that we need to take `embcat.T` and matrix multiply it with `dhprebn`\n",
        "  * to get `db1` which is an addition, we just need to sum the elements in `dhprebn` along some dimension. To make the dimensions work, we need to sum along the 0th axis to eliminate a dimension and we `keepdims=False` because we want to just get a single 1D lecture of 64"
      ],
      "metadata": {
        "id": "amNiAygn5gE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6 Backpropogating through `emb` and `C`\n",
        "\n"
      ],
      "metadata": {
        "id": "u7__7WDR6z0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "embcat = emb.view(emb.shape[0], -1) # forward pass\n",
        "```\n",
        "* we have the derivative of `embcat` and now want to backpropogate into `emb`\n",
        "* shapes:\n",
        "  * embcat.shape = [32, 30]\n",
        "  * emb.shape = [32, 3, 10]\n",
        "* this layer in the forward pass did the concantenation of these 10D character vectors and so now we just want to undo that. This is very relatively easy:\n",
        "  * `view` is just a representation of an array. It's just a logical form of how you interpret the array so let just re-interpret to see what it was before\n",
        "* `demb` is not [32, 30] but is rather `dembcat` when viewed as the original shape which is `emb.shape` -> you can pass tuples into view\n",
        "\n",
        "* we just need to backprop through the indexing operation of:\n",
        "```python\n",
        "emb = C[Xb] # embedding the characters into vectors\n",
        "```\n",
        "* shapes:\n",
        "  * emb.shape = [32, 3, 10] (32 examples, 3 characters, each character has a 10D embedding) -> achieved by taking the lookup table C which have 27 possible characters each of 10D\n",
        "  * C.shape = [27, 10]\n",
        "  * Xb.shape = [32, 3] -> looked up the rows specificied inside the tensor `Xb`, for each examples it give us the identity or the index of which character is part of that example\n",
        "* what's happening here is that there are integers inside `Xb` and each one of these integers is specifying which row of `C` we want to pluck out and then we arrange these rows that we've plucked out into [32, 3, 10] tensor and we package them into this tensor. For every one of these plucked out rows we have their gradients now but they are arranged in the [32, 3, 10] tensor. All we have to do now is we just need to route this gradient backwards through this assignment so we need to find which row of `C` that every one of these 10D embeddings come from and then we need to deposit  them into `dC`. So, we just need to undo the indexing and if any of these rows of `C` was used multiple times which almost certainly is the case, like row one, then we need to remember that the gradients that arrive there have to add so for each occurence we need to have an addition\n",
        "* a better approach than a for loop would be a vectorized efficient operation\n",
        "* `dC = torch.zeros_like(C)` to initialzie [27,10] tensor of all zeros\n",
        "* the for loop will then iterate over all the elements of `Xb`, then let's get the index at that specific position. The index is `Xb[k,j]`\n",
        "  * now in the forward pass we took the row of `C` at index and deposited it into `emb` of k or j\n",
        "  * hence, we need to go backwards and we just need to route `demb` at the position k or j. We now have the derivatives for each position and it's 10D and it just need to go into the correct row of `dC` -> we add as well as there can be multiple occurences as the same row could have been used multiple times and all those derivatives will go back through the indexing and add\n"
      ],
      "metadata": {
        "id": "0KwnzU8m8GNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass: emb = C[Xb]\n",
        "print(emb.shape, C.shape, Xb.shape)\n",
        "print(Xb[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWRI6rBd9Gqk",
        "outputId": "db121d33-f849-4fc9-bf26-c99082184948"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 10]) torch.Size([27, 10]) torch.Size([32, 3])\n",
            "tensor([[ 1,  1,  4],\n",
            "        [18, 14,  1],\n",
            "        [11,  5,  9],\n",
            "        [ 0,  0,  1],\n",
            "        [12, 15, 14]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.7 overall implementation"
      ],
      "metadata": {
        "id": "w8RKtl3p9a9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "dlogprobs = torch.zeros_like(logprobs) # array of zeros exactly in the shape of logprobs\n",
        "dlogprobs[range(n), Yb] = -1.0/n\n",
        "dprobs = (1.0 / probs) * dlogprobs\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "dnorm_logits = counts * dcounts\n",
        "dlogits = dnorm_logits.clone()\n",
        "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "dhpreact = (1.0 - h**2) * dh\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "dbnraw = bngain * dhpreact\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "dbndiff = bnvar_inv * dbnraw\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "dbndiff += (2*bndiff) * dbndiff2\n",
        "dhprebn = dbndiff.clone()\n",
        "dbnmeani = (-dbndiff).sum(0)\n",
        "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "demb = dembcat.view(emb.shape)\n",
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "  for j in range(Xb.shape[1]):\n",
        "    ix = Xb[k,j]\n",
        "    dC[ix] += demb[k,j]\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a3NAoPloc94",
        "outputId": "47ab18fe-775d-4ca5-9955-f7b6a06b2f34"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "W1              | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n",
            "b1              | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 1.3969838619232178e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 6.05359673500061e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Exericise 2: cross entropy loss backward pass\n",
        "\n"
      ],
      "metadata": {
        "id": "ygSL4qssnpCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* we were doing way too much work in exercise 1. We actually won't do that in practice because we broke up the loss layer into very tiny layers. However, when we actually write it down and check each step for the loss, we realize that a few terms cancel out each other and simplify, hence, the mathematical statement we end up with in the end can be much easier than the statements of loss we have in the beginning\n",
        "* in `F.cross_entropy()` you just pass in the logits and the labels and you get the exact same loss\n",
        "* `fast_loss()` coming as a chunk of expressions as a single mathematical expression is the same but it's much faster in the forward pass. It's also much faster in the backward pass and the reason for that is if you look at the mathematical form of this and differentiate again you will end up with a very small and short expression.\n",
        "* That's what we want to do here, we want to in a single operation go directly into `dlogits` and we need to implement `dlogits` as a function of `logits` and `Yb` and it will be significantly shorter than what we did in exercise 1\n",
        "* look at what is the mathematical expression of the loss and differentiate w.r.t the logits\n",
        "* what's happening here is we have `logits`, then there's a `softmax` that takes in the `logits` and gives the `probs`. Then we are using the identity of the correct next character to pluck out a row of `probs`, take a `-log()` of it to get out negative log `probs` and then we average all the `-log().probs()` to get out loss.\n",
        "  * So what we have is for a single individual example, we have that:\n",
        "  ```python\n",
        "  loss = -log(Py) # p = vectors of all the probs where y is the label\n",
        "  ```\n",
        "    * p = `softmax` so the ith component of p of this `probs` vector is just the `softmax` function so raising all the `logits` to the power of e and normalizing so everything somes to 1\n",
        "    * we are then interested in the derivative of the loss w.r.t ith logi. We have the lth indexed with the specific label y and on the bottom over j of e to Lj and the negative log of that -> deriving the dloss by `dLi` and then implement it in the forward pass\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/Raahim58/Neural-networks/main/images/loss%201.png\" height=300 width=300>\n",
        "\n",
        "    <img src=\"https://raw.githubusercontent.com/Raahim58/Neural-networks/main/images/loss%202.png\" height=300 width=300>\n",
        "\n",
        "    * math to derive the gradients analytically: <img src=\"https://raw.githubusercontent.com/Raahim58/Neural-networks/main/images/loss%203.png\" height=300 width=300>\n",
        "\n"
      ],
      "metadata": {
        "id": "zJAzcBZWSaLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HirqjNlMAOdD",
        "outputId": "8f2a19d8-2688-469c-f9a0-e0d0c02c6484"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.340137481689453 diff: -2.384185791015625e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "\n",
        "dlogits = F.softmax(logits, 1) # softmax alongside the rows\n",
        "dlogits[range(n), Yb] -= 1\n",
        "dlogits /= n\n",
        "\n",
        "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6.9e-9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H9LUI07FOf2",
        "outputId": "b998ff85-b545-4aa7-9dc8-5058d9482ead"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 9.778887033462524e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* we need to understand `dlogits`:\n",
        "  * we have a batch of 32 examples of 27 characters\n",
        "  * `dlogits` is the probabilites that the probability matrix has in the forward pass but then here these black squares are the positions of the correct indices where we subtracted a 1. So, what is this doing? These are the derivatives on the `logits`\n",
        "  * looking at the first row to get the probabilties, and the logits without being scaled by n, we see it is exactly equal to the probability but then the position of the correct index has a -=1. Notice that if you take `dlogits[0].sum()`, it sums to 0 so we should think of these gradients at each cell like a force. We are going to be basically pulling down on the `probs` of the incorrect characters and we are going to be pulling up on the `probs` of the correct characters. The amount of push and pull is equalized since the sum is 0 so the amount to which we pull down on the `probs` and the amount we pull up on the `probs` is equal.\n",
        "  * Think of the neural net like pulley system now: We're up here on top of the logits and we're pulling down the `probs` of incorrect indices and pulling up the `probs` of the correct indices and in this complicated pulley system where everrything is mathematically determined, just think of this tension translating to this complicating pulling mechanism and then eventually we get a tug on the weights and the biases. Basically in each update we just kind of like tug in the direction we like for each of these elements and the parameters are slowly given in to the tug and that's what training a neural net at a high level looks like. So, the force of push and pull in these gradients are very intuitive. We're pushing and pulling on the incorrect and correct answers respectively, and the amount of force we're applying is actually proportional to the `probs` that came out in the forward pass. For e.g if our `probs` came out exactly correct then they would have 0 everywhere except for one at the correct position then the logits will all be a row of zeros for that examples, there would be no push and pull.\n",
        "  * The amount to which your prediction is incorrect is exactly the amount by which you're going to get a pull or a push in that dimension. So if for e.g you have a very confidently mispredicted element, then what's going to happy is that that element is going to be pulled down very heavily and the correct answer is going to be pulled up by the same amount, and the other characters are not going to be influenced too much. The amount to which you mis predict is proportional to the strength of the pull, and that's happening independently in all the dimensions of this tensor -> magic of `cross_entropy_loss()` and what it's doing fynamically in the backward pass of the neural net.\n",
        "  "
      ],
      "metadata": {
        "id": "qndVaPYfSAR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape, Yb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnKs6zNOFPzu",
        "outputId": "eda31149-9608-4e63-cc2c-1ca65b5c6914"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]), torch.Size([32]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(logits, 1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXKop5sDFPxB",
        "outputId": "919d710b-a318-4037-d72b-67661f0ddda1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0693, 0.0914, 0.0185, 0.0486, 0.0199, 0.0858, 0.0243, 0.0353, 0.0179,\n",
              "        0.0287, 0.0377, 0.0362, 0.0367, 0.0291, 0.0346, 0.0143, 0.0095, 0.0185,\n",
              "        0.0157, 0.0542, 0.0508, 0.0206, 0.0254, 0.0709, 0.0592, 0.0245, 0.0224],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits[0] * n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uwt_kVBFPuJ",
        "outputId": "339bca0e-b3b5-4620-8b65-98fb695383f9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0693,  0.0914,  0.0185,  0.0486,  0.0199,  0.0858,  0.0243,  0.0353,\n",
              "        -0.9821,  0.0287,  0.0377,  0.0362,  0.0367,  0.0291,  0.0346,  0.0143,\n",
              "         0.0095,  0.0185,  0.0157,  0.0542,  0.0508,  0.0206,  0.0254,  0.0709,\n",
              "         0.0592,  0.0245,  0.0224], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits[0].sum();"
      ],
      "metadata": {
        "id": "HAlNL9XKFU0R"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(dlogits.detach(), cmap='gray');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xfR1ThbyFW0r",
        "outputId": "5b8952a4-184f-401c-aaaa-662ff13fe4d6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Exercise 3: batchNorm layer backward pass\n",
        "\n",
        "* going to do the same thing for batchNorm we did for cross entropy loss in exercise 2. We're going to consider it as aglued single mathematical expression and back propogate through it in an efficient manner because we are going to derive a much simple formula for batch normalization\n",
        "* in exercise 1 we broke down the batchNorm layer into pieces and and all the atomic operations inside and then we backpropogated through it one by one. Now, we just have a single sort of forward pass of a batchNorm and it's all glued together and we see that we get the same result as before. For backward pass we would also like to implement a single formula for backpropogating through this entire operation which is the batch normalization"
      ],
      "metadata": {
        "id": "oC_gdA8xSSS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 forward pass\n",
        "\n",
        "* in the forward pass previously, we took `hprebn`, the hidden states of the pre-batch normalization, and created `hpreact`, the hidden states just before the activation.\n",
        "* in the batchNorm layer, `hprebn` is X and `hpreact` is Y. So, in the backward pass what we would like to do now is we have `dhpreact` and we would like to produce `dhprebn`, and we would like to do that in a very efficient manner.\n"
      ],
      "metadata": {
        "id": "a__d3tobq33p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI8CVEnpSW1C",
        "outputId": "b7bc8288-48de-4348-f859-9b3b9a9852f5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 backward pass\n",
        "\n",
        "* calculate `dhprebn` given `dhpreact`\n",
        "* for the purposes of this exercise we're going to ignore gamma and beta and their derivatives because they take on a very simple form in a very similar way as to what we did above.\n",
        "* Follow the order of the numbers to backpropogate -> backpropogate into Xhat, then into var (sigma**2), then into mean (meu u) and then into X. Just like a topological sort in micrograd, we will go from right to left. We're doing the exact same thing except we're doing it with symbols and on a piece of paper.\n",
        "\n",
        "  <img src = \"https://raw.githubusercontent.com/Raahim58/Neural-networks/main/images/batchnorm.png\" height = 400 width = 300>\n",
        "\n",
        "  <img src = \"https://raw.githubusercontent.com/Raahim58/Neural-networks/main/images/batchnorm%202.png\" height = 400 width = 300>\n",
        "\n",
        "* eq1: we have dL/dYi and we want dL/dXi for all the i's in these vectors. We have to be careful because these are vectors, there are many nodes here inside X, Xhat, and Y. However, mean (meu u) and var (sigma**2) are just individual scalars, single numbers. We have to imagine there are multiple nodes here or we will get out math wrong.\n",
        "\n",
        "* eq2: there are many excess Xhats here and sigma^2 (var) is just a single individual number here. When we look at the expression for dL/dsigma^2, we have to actually consider all the possible paths that Xhat depends on sigma^2. Hence sigma^2 has a lot of fan outs, there's a lot of arrows coming out from sigma^2 into all the Xhats.\n",
        "  * then there's a backpropogating signal from each Xhat into sigma^2 and that's why we need to sum over all those i's from i=1 to m, of the dL/dXihat which is the global gradient * XiHat/dSigma^2 (local gradient of operation 2). Then mathematically we solve it for eq 2 and we get a certain expression for dL/dsigma^2. We're going to be using this expression when we back propogate into mean (meu u) and then eventually into X.\n",
        "\n",
        "* eq3: dL/dmu(u) continuing our backpropogation into mean (meu u). What is dL/dmu. We need to be careful again that mu influences Xhat and Xhat is actually lots of values. So for e.g, our mini batch size is 32, then Xhat is 32 numbers and 32 arrows going back to mu, and then mu going to sigma^2 is just a single arrow since sigma^2 is a scalar. In total there are 33 arrows emanating from `mu`, and then all of them have gradients coming into `mu` and they all need to be summed up.\n",
        "  * We sum up all the gradients of dL/dXihat * dXihat/dmu (32 arrows) + dL/dsigma^2 * dsigma^2/dmu (1 arrow). We work out the expression mathematically\n",
        "  * 1st term is simplified easily but for the 2nd term when we look at dsigma^2/dmu and then we simplify, at one point if we assume in a specialcase that mu is actually the average of Xi's as it is in this case, then we plug it in then gradient vanishes and becomes 0. This makes the entire second term cancel. In the special case where mu = average as it is in the case of batch normalization, the gradient will vanish and become 0. The whole term cancels and you get a straight forward expression for dL/dmu\n",
        "\n",
        "* eq4: deriving dL/dXi which is ultimately what we're after.\n",
        "  * Let's count how many numbers are there inside X. As i mentioned there are 32 numbers, 32 little Xi's. The number of arrows emanting from each Xi -> there's an arrow going to mu, and arrow going to sigma^2, and an arrow going to Xhat\n",
        "  * scrutinizing the arrow going from each Xi to Xhat: each Xi hat is just a function of Xi and all the other scalars. So, Xihat only depends on Xi and none of the other X's. Therefore there are in this single arrow actually 32 arrows but those 32 arrows are going parallel without interfering, going between X and Xhat.\n",
        "    * How many arrows are emanating from each Xi? there are 3 arrows: mu, sigma^2, and the associated Xhat. So in backpropogation we now need to apply the chain rule and we need to add those contributions:\n",
        "      * we're chaining through mu, sigma^2 and Xhat. We already have the derivatives of all these 3 in the other 3 eq's. What we need now is the other 3 terms associated with the 3 expressions we got in the first 3 eq's\n",
        "      * differentiating each of those expressions w.r.t Xi\n",
        "      * gets a little bit more tricky when we add up it all together: we get a very big expression.\n",
        "      * we need to be careful here working with dL/dXi for specifice i here, but when we are plugging in some of these terms like dL/dsigma^2, we need to not use i from the other eq's because the i is like running in the for loop for that specific equation which is different from the i in eq4. Hence, instead we use a j and make sure that the j is not the i we use in other eq's like eq2. The j is like a little local iterator over 32 terms. Some of these are hence j's and some are i's. Some terms cut out and you can just cancel them\n",
        "      * when you simplify the expression you realize some of the terms that are coming out are just the Xihat's so you can simplify just by rewriting that.\n",
        "  * the final expression use the stuff we have and it derives the thing we need so we have dL/dY for all the i's, and those are used plenty of times here. Also, i addition we are using Xihat's and Xjhat's, and they just come from the forward pass. Otherwise, this is just a simple expression and it gives us dL/dXi for all the i's which is what we're ultimately intersted in.\n",
        "  * final backward pass implemented as a single expression. Getting this expression was not trivial and there's a lot going on pakced into this one formula and this is a whole exercise by itself. You have to consider the fact that the final expression derived is just for a single neuron and a batch of 32 examples. We actually have 64 neurons, so this layer has toe valuate the batchNorm backward pass for all those 64 neurons in parallel independely. This has to basically happen in all the column of inputs in the single backward pass code.\n",
        "  * in addtion to that, we have a lot of sums in the final derived expression so we need to make sure that when we do those sums, we broadcast them correctly onto everything else here. Verify in the end with PyTorch itself in the end."
      ],
      "metadata": {
        "id": "m4PvJWiOq8Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsVyg6GNUB70",
        "outputId": "fb4cec46-ec31-4a52-8374-9c80fc0f9fe0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dhprebn.shape, bngain.shape, bnvar_inv.shape, dbnraw.shape, dbnraw.sum(0).shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovMRO7G9UCyO",
        "outputId": "f6f6379b-782c-478d-8e7d-3285e7bffa2c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 64]),\n",
              " torch.Size([1, 64]),\n",
              " torch.Size([1, 64]),\n",
              " torch.Size([32, 64]),\n",
              " torch.Size([64]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Putting it all together\n",
        "\n",
        "* if you want to compare PyTorch backward pass with manual backprop then uncomment a few things as seen in code and run compare in the next code cell"
      ],
      "metadata": {
        "id": "hVFmEij5YIeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "with torch.no_grad(): # remove when comparing efficiency\n",
        "\n",
        "  # kick off optimization\n",
        "  for i in range(max_steps):\n",
        "\n",
        "    # minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the characters into vectors\n",
        "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    # Linear layer\n",
        "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "    # BatchNorm layer\n",
        "    # -------------------------------------------------------------\n",
        "    bnmean = hprebn.mean(0, keepdim=True)\n",
        "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "    hpreact = bngain * bnraw + bnbias\n",
        "    # -------------------------------------------------------------\n",
        "    # Non-linearity\n",
        "    h = torch.tanh(hpreact) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "\n",
        "    # loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "    # manual backprop! #swole_doge_meme\n",
        "    # -----------------\n",
        "    # cross entropy loss\n",
        "    dlogits = F.softmax(logits, 1)\n",
        "    dlogits[range(n), Yb] -= 1\n",
        "    dlogits /= n\n",
        "    # 2nd linear layer\n",
        "    dh = dlogits @ W2.T\n",
        "    dW2 = h.T @ dlogits\n",
        "    db2 = dlogits.sum(0)\n",
        "    # tanh\n",
        "    dhpreact = (1.0 - h**2) * dh\n",
        "    # batchnorm backprop\n",
        "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "    # 1st linear layer\n",
        "    dembcat = dhprebn @ W1.T\n",
        "    dW1 = embcat.T @ dhprebn\n",
        "    db1 = dhprebn.sum(0)\n",
        "    # embedding\n",
        "    demb = dembcat.view(emb.shape)\n",
        "    dC = torch.zeros_like(C)\n",
        "    for k in range(Xb.shape[0]):\n",
        "      for j in range(Xb.shape[1]):\n",
        "        ix = Xb[k,j]\n",
        "        dC[ix] += demb[k,j]\n",
        "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "    # -----------------\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "    for p, grad in zip(parameters, grads):\n",
        "      # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())\n",
        "\n",
        "  #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "  #     break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EaBRUicYNAv",
        "outputId": "199922ef-eadb-4215-f1ef-df386189b472"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.7562\n",
            "  10000/ 200000: 2.1635\n",
            "  20000/ 200000: 2.3736\n",
            "  30000/ 200000: 2.4067\n",
            "  40000/ 200000: 1.9977\n",
            "  50000/ 200000: 2.3930\n",
            "  60000/ 200000: 2.3773\n",
            "  70000/ 200000: 2.0889\n",
            "  80000/ 200000: 2.3315\n",
            "  90000/ 200000: 2.1295\n",
            " 100000/ 200000: 1.9589\n",
            " 110000/ 200000: 2.3145\n",
            " 120000/ 200000: 2.0229\n",
            " 130000/ 200000: 2.4394\n",
            " 140000/ 200000: 2.3821\n",
            " 150000/ 200000: 2.0920\n",
            " 160000/ 200000: 1.9606\n",
            " 170000/ 200000: 1.8074\n",
            " 180000/ 200000: 1.9932\n",
            " 190000/ 200000: 1.9223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# useful for checking your gradients\n",
        "for p,g in zip(parameters, grads):\n",
        "  cmp(str(tuple(p.shape)), g, p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0_MsLhhYQlr",
        "outputId": "78eccbc8-2127-4340-f82e-268840b01cd7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27, 10) approximate, maxdiff=1.4901161193847656e-08\n",
            "(30, 200) approximate, maxdiff=5.587935447692871e-09\n",
            "(200,) approximate, maxdiff=3.4924596548080444e-09\n",
            "(200, 27) approximate, maxdiff=1.4901161193847656e-08\n",
            "(27,) approximate, maxdiff=8.381903171539307e-09\n",
            "(1, 200) approximate, maxdiff=2.3283064365386963e-09\n",
            "(1, 200) approximate, maxdiff=3.725290298461914e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "compared results with one iteration (break result):\n",
        "\n",
        "* (27, 10) approximate, maxdiff=1.4901161193847656e-08\n",
        "* (30, 200) approximate, maxdiff=5.587935447692871e-09\n",
        "* (200,) approximate, maxdiff=3.4924596548080444e-09\n",
        "* (200, 27) approximate, maxdiff=1.4901161193847656e-08\n",
        "* (27,) approximate, maxdiff=8.381903171539307e-09\n",
        "* (1, 200) approximate, maxdiff=2.3283064365386963e-09\n",
        "* (1, 200) approximate, maxdiff=3.725290298461914e-09"
      ],
      "metadata": {
        "id": "nugizFiX_zDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
      ],
      "metadata": {
        "id": "gB6Hrhf8YUIv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7jLR0YuYWee",
        "outputId": "f81961a1-f201-4118-893e-ef271c6f1996"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.0720419883728027\n",
            "val 2.1110339164733887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "got:\n",
        "* train 2.0720419883728027\n",
        "* val 2.1110339164733887"
      ],
      "metadata": {
        "id": "SPKw0dYp_9zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # ------------\n",
        "      # forward pass:\n",
        "      # Embedding\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # ------------\n",
        "      # Sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPXDb1b4YZQG",
        "outputId": "4da1d8e2-d1b2-4a5f-9a6a-66a5f67dda72"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mora.\n",
            "mayah.\n",
            "see.\n",
            "mad.\n",
            "ryla.\n",
            "reisa.\n",
            "jendraeg.\n",
            "adelynnelin.\n",
            "shy.\n",
            "jen.\n",
            "eden.\n",
            "estanaraelynn.\n",
            "hoka.\n",
            "cayshabergihimie.\n",
            "trickennellennie.\n",
            "cayus.\n",
            "macder.\n",
            "yarulyeha.\n",
            "kayshayveyah.\n",
            "hal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary"
      ],
      "metadata": {
        "id": "gpa3eeHNYKrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* manuall backproping gave us a diversity of layers to backpropogate through and it gives a comprehensive sense of how these backward passes are implemented and how they work\n",
        "* you'd be able to derive them yourself but in practice you probably don't want to and just want to use the python autograd\n",
        "* we have some intution of how grad flows backwards through the neural net starting from the loss and how they flow through all the variables and all the intermediate results and if you understood a good deal of it, consider yourself as the buff dog!\n",
        "* next lecture will contains RNN's, LSTM, GRU. We will start to complexify the architecture and start to achieve better log likelihoods.\n",
        "\n",
        "  <img src = \"https://raw.githubusercontent.com/Raahim58/Neural-networks/main/images/summary.png\" height = 500 width = 500>\n"
      ],
      "metadata": {
        "id": "0cgyzPbeYL7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra curriculum\n",
        "\n"
      ],
      "metadata": {
        "id": "iRDhMDgO9Boa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Yes you should understand backprop: https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b\n",
        "* BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
        "* Bessel’s Correction: http://math.oxford.emory.edu/site/mat...\n"
      ],
      "metadata": {
        "id": "6tFGlTYEASkL"
      }
    }
  ]
}