{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFAlxeZm674AfhJhl0z1h7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raahim58/Neural-networks/blob/main/07_building_GPT_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 07. Building Makemore from scratch (Part5: Building a WaveNet)\n",
        "\n",
        "**Resources:**\n",
        "\n",
        "* tutorial lecture 7 code:\n",
        "  * https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing\n",
        "  * https://github.com/karpathy/ng-video-lecture\n",
        "\n",
        "* nanoGPT github repo: https://github.com/karpathy/nanoGPT\n",
        "\n",
        "* link to youtube lecture 7: https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=7\n",
        "\n",
        "* whole lecture series code: https://github.com/karpathy/nn-zero-to-hero\n",
        "\n",
        "**Extra curriculum:**\n",
        "\n",
        "* MLP model based on 2003 paper: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
        "\n",
        "* WaveNet 2016 from DeepMind https://arxiv.org/abs/1609.03499\n",
        "\n",
        "* Attention is All You Need paper: https://arxiv.org/abs/1706.03762\n",
        "\n",
        "* OpenAI GPT-3 paper: https://arxiv.org/abs/2005.14165\n",
        "\n",
        "* OpenAI ChatGPT blog post: https://openai.com/blog/chatgpt/\n",
        "\n",
        "* 3Blue1Brown *What is a GPT?*: https://youtu.be/wjZofJX0v4M?si=0VFIijzi6P-9wH_F\n",
        "\n",
        "* 3Blue1Brown *Self-attention in transformers*: https://youtu.be/eMlx5fFNoYc?si=AXvbawHAa8h4K8Fh"
      ],
      "metadata": {
        "id": "bxCONftpWTBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Getting setup + intro\n",
        "\n",
        "<img src = \"https://raw.githubusercontent.com/Raahim58/Neural-networks/main/images/transformer.png\" height = 600 width = 600>\n",
        "\n",
        "* chatGPT is a language model because it models the sequence of words or characyers or tokens more generally, and it knows how sort of words follow each other in English language. From its perspective it's completing a sequnce with the outcome\n",
        "* we'll be focusing on the under the hood stuff that makes chatGPT work:\n",
        "  * what is the neural network that models the sequence of these words under the hood? -> this comes from the paper \"Attention all you need\" that proposed the transformer architecture\n",
        "  * GPT = generatively pre-trained transformer, where transformer is the neural net that actually does all the heavy lifting under the hood.\n",
        "  * we're not going to be re-producing chatGPT as it is a very serious production grade system which is trained on a good chunk of internet and then there's a lot of pre-training and fine-tuning stages to it.\n",
        "  * what we're going to do be focusing on is a to train a transformer based language model. In our case it will be a character language mode which will be trained on a smaller chunk of dataset namely \"tinyShakespeare.txt\" -> concanetenated of all of the works of Shakespeare\n",
        "  * we will model how the characters follow each other\n",
        "  * it will try to produce character sequences that look like the Shakespeare text file where in reality it produces on a token by token basis for chatGPT\n",
        "    * token are sub word pieces, so they're like word chunk level\n",
        "  * nanoGPT repo for training a transformer on any given text\n",
        "    * `train.py` is a ~300-line boilerplate training loop and `model.py` a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI.\n",
        "* what we will do:\n",
        "  * define a transformer piece by piece\n",
        "  * train it on the `tinyShakespeare.txt` dataset\n",
        "  * see how we can generate infinte Shakespeare\n",
        "  * can copy to any other dataset we like\n",
        "* preliminary-code:\n",
        "  * we sort the characters uniquely\n",
        "  * the number of them is going to be our vocab size -> the possible elements of our sequences.\n",
        "  * we get 65 characters in total with capitals, lowercase, and special letters.\n"
      ],
      "metadata": {
        "id": "vcb3jTv5XvW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UjPx-MzX2fd",
        "outputId": "5d1af826-932b-4f5a-f2e6-ce5bb7d299a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-08 12:49:34--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-09-08 12:49:34 (25.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "hNvF-XJ6OIMt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bymJa-ilOKeE",
        "outputId": "62516fe5-5709-4e66-e0ad-cba172fb7984"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVUA5ZFGOSHP",
        "outputId": "07b981cf-cb12-4ffc-da51-b893106cb96c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters sorted that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBURjND4OUQk",
        "outputId": "bf9baf10-2927-453c-cf00-d3640a4819ea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Tokenization + split dataset into train/val\n",
        "\n",
        "* tokenizing the input text means to conver the raw text as the string to some sequence of integers according to some vocabulary of possible elements\n",
        "* since we're building a character level language model, we're just going to be translating individual characters into integers\n",
        "\n",
        "* to tokenize we:\n",
        "  * we're going to be using an encoder and a decoder\n",
        "    * an encoder takes an input text and returns a list of integers that represent that string\n",
        "    * decoder takes the list of integers to map it back to the input string of characters/words\n",
        "  * we iterate over all the characters and create a lookup table from the character to the integer and vice versa\n",
        "  * to encode some string, we translate all the characters individually and to decode it back we use the reverse mapping and concatenate all of it\n",
        "\n",
        "* there is one of many possible encodings or tokenizers and it's a very simple one\n",
        "  * there are many other schemas which people have come up with, for e.g Google will use a \"SentencePiece\" which encodes text into integers but in a different schema and using a different vocabulary\n",
        "    * \"SentencePiece\" is a sub word sort of tokenizer -> we're not encoding entire words but we're also not encoding indiviudal characters, it's a subword unit level\n",
        "      * that's what is usally adopted in practice 8\n",
        "    * openAI library called \"ticktoken\" which uses a byte pair encoding tokenizer and that's what GPT uses\n",
        "      * it has 50526 tokens\n",
        "      * so you can basically trade off the code book size and the sequence lengths so you can have a very long sequence of integers with very small vocabularies or you can have a short sequence of integers with very large vocabularies\n",
        "  * typically people use these sub word encodings in practice but we're like to keep our tokenizer very simple so we'll be using characterlevel tokenzier so that means we have very small code books, we have very simple encode and decode functions, but we do get very long sequences as a result but we're going to stick with that method in this notebook because it's the simplest thing\n",
        "* what we're doing here to tokenize:\n",
        "  * we use `torch.tensor` from the PyTorch library\n",
        "  * we will take all of the text in the `tinyShakespeare.txt` file and encode it, and then wrap it in a `torch.tensor` to get the data tensor\n",
        "  * we get a sequence of integers which are an identical translation of the characters themselves\n",
        "  * e.g 0 is a new line character, 1 is a space\n",
        "* we all split our dataset into 90% training dataset and 10% validation dataset\n",
        "  * we do this because it helps us understand to what extent our model is overfitting so we're going to basically hide and keep the validation data on the side because we don't want just a perfect memorization of this exact text file, we rather want a neural network that sort of creates a shakespeare like text and so it should be fairly likely for it to produce the actual stowed away true shakespeare text  "
      ],
      "metadata": {
        "id": "jKRmXXZPOd9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uLRsKWOPYVo",
        "outputId": "7c55e321-7199-4569-c2c0-9509274dd6e6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm1HHhlWPaaQ",
        "outputId": "adc49c37-ae72-4b0c-b1ed-6204f8a7c63d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest 10% val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "LtuqRq4tR9Rx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataloader: batches of chunks of data\n",
        "\n",
        "* we would like to start plugging in the text sequences or integer sequences into the transformer so that it can train and learn those patterns\n",
        "* an important thing to realize is that we're never going to actually feed the entire text into the Transformer all at once, that would be computationally very expensive and prohibitive. When we actually train a transformer on a lot of these dataset, we only work with chunks of the dataset and when we train the Transformer we basically sample random little chunks out of the training set and train them just chunks at a time. These chunks have some kind of a maximum length\n",
        "  * the maximum length in the code is called `block_size` but can also be known as `context_length` elsewhere\n"
      ],
      "metadata": {
        "id": "bYsN_QKlSnaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first 9 characters in the training set sequence\n",
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ITG4NgpUHMf",
        "outputId": "dafb4746-f725-45e0-f5e4-3cc4db2c9e1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# showing the prediction of integers/characters in the sequence\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1hoxfWnUMQt",
        "outputId": "4b1b94ef-98d2-40e1-c098-26b722147709"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* when we sample a chunk of data like below for the 9 characters out of the training set, this actually has multiple examples packed into it.That's because all of these characters follow each other and so what these sequence of integers we get are going to say when we plug them into the Transformer is we're going to simultaneously train it to make a prediction at every one of its positions\n",
        "  * now in a chunk of 9 characters, they're actually 8 individual examples packed in there\n",
        "  * `x` are the inputs to the transformer. They will just be the first block size characters\n",
        "  * `y` will be the next `block_size` character so it's offset by one, that's because `y` are the targets for each position in the input\n",
        "  * we then iterate over the `block_size` of 8, and the `context` is always all the characters in X up to `t` and including `t`, and the target is always the `t`th character, but in the targets array `y`.\n",
        "  * another thing to mention is that when we train on all the 8 examples with context between 1 and `block_size`, we train on that not just for computational reasons because we happen to have the sequence (not just for efficiency), it's also done to make the Transformer network be used to seeing contexts from all as little as 1 to `block_size`. We'd like the transform to be used to seeing everything in between and that's going to be useful later during inference because while we're sampling, we can start the sampling generation with as little as 1 character of context and the Transformer knows how to predict the next character with all the way upto just context of 1 adn so then it can predict everything up to `block_size`, and after `block_size` we have to start truncating because the Transformer will never receive more than `block_size` inputs when it's predicting the next character\n",
        "* we've looked at the time dimension of the tensors that are going to be feeding into the Transformer, there's one more dimension to care about and that's the batch dimension\n",
        "  * so as we're sampling these chunks of text, everytime we're going to feed them in a Transformer, we're going to have many batches of multiple chunks of text that are all stacked up in a single tensor. That's just done for efficiency so we can keep the GPU's busy because they're very good at parallel processing of data so we just want to process multiple chunks all at the same time but those chunks are processed completely independently, they don't talk to each other. Let's generalize the code and introduce a batch dimension in the code below:\n",
        "   "
      ],
      "metadata": {
        "id": "ZogfaMwHVg3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337) # for reproducibility\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6d6wHh_VX4y",
        "outputId": "3df5d2b3-f1af-4b1d-e482-0d9daf3df4a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* to get the batch for an arbitrary split:\n",
        "  * if the split is a training split, we just look in the training dataset otherwise the val data giving us the `data` array.\n",
        "  * then we generate random positions in `ix`, we actually generate batch size number of random offsets into the training sets\n",
        "  * `x`'s are the first block size characters starting at `i`, `y`'s are offset by 1 of that, so just add plus 1 -> we're going to get those chunks for every one of integers `i` in `ix` and use `torch.stack` to take all those 1D tensors and we're going to stack them up at rows, so all they become a row in a [4, 8] tensor (with each row being a chunk in the training dataset) -> the total 32 examples are completely independent as far as the Transformer is concerned\n",
        "    * the target's in the associated `y`'s will come in through the Transformer all the way at the end to create the loss function so they will give us the correct answer for every single position inside `x`\n",
        "* so essentially we have 32 examples packed into a single batch of the input `x` and then the desired targets in `y` so now this integer tensor of `x` is going to feed into the Transformer, and that Transformer is going to simultaneously process all these examples and then look up the correct integers to predict in each one of these positions in the tensor `y`"
      ],
      "metadata": {
        "id": "UokQOMIiEK86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Simplest baseline: bigram language model"
      ],
      "metadata": {
        "id": "LUYUqJlKHO1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Building the model, loss, generation\n",
        "\n",
        "* the model:\n",
        "  * when the inputs and targets come in `forward`, we just take the index of the inputs `x` which we renamed to `idx`, and we pass them into the token embedding table:\n",
        "    * in the `init` constructor, we're creating a token embedding table, and it is of size [vocab_size, vocab_size], and we're using `nn.Embedding` which is a very thin wrapper around basically a tensor of shape [vocab_size, vocab_size].\n",
        "    * when we pass `idx` in the `forward` for logits, every single integer in our input is going to refer to the embedding table and is going to pluck out the row of that embedding table corresponding to its index (integer 24 will pluck out the row 24). Pytorch will arrange all of this into a [B, T, C] -> [4, 8, 65] (batch, time, channel) tensor, and will interpret them as logits which are basically the scores for the next character in the sequence. We are predicting what comes next based on just the individual identity of a single token and you can do that because the token are currently not talking to each other, and they're not seeing any context except for just seeing themselves -> `print(out)` gets us the prediction, the scores or logits, for every one of the [4, 8] positions\n",
        "\n",
        "* evaluating the loss of our function:\n",
        "  * in makemore series, we saw that a great way to measure the loss or quality of our predictions is to use the negative loss likelihood: `Cross_Entropy` -> loss is the cross entropy on the predictions and the targets (so loss measures the quality of the logits w.r.t Targets, or in other words how well are we predicting the next character based on `logits`).\n",
        "    * intuitively, the correct dimension of the logits depending on whatever the target is should have a very high number and all the other dimensions should be very a low number\n",
        "  * if we have a multi-dimensional input, pytorch wants [B, C, T] instead of [B*T, C] in `cross_entropy` -> so we'll reshape those logits by unpacking the numbers, and then B*T for the first dimension. We will take all the positions for input tensor and then stretch them out in a 1D sequence and preserve the channel dimension as the second dimension (we're just stretching the array so it's 2D and in that case it's going to be better conform to what pytorch sort of expects)\n",
        "  * we have to do the same with `targets` as we did with `logits` because currently `targets` are of the shape [B, T] and we just want it to be [B*T]. Alternatively, we could do -1 because pytorch will guess what it should be if you want to lay it out.\n",
        "  * since the data is equally likely, we expect the loss to be (-ln(1/65)) ~ 4.13 instead of the 4.87 we're getting -> our predicitons are niot super diffuse but we've got a little bit of entropy and so we're guessing wrong\n",
        "\n",
        "* generating from the model:\n",
        "  * we take the same kind of input `idx` which is the current context of characters in some batch, so it's also [B,T]\n",
        "  * the job of `generate` is to basically take the [B,T] and extend it to [B, T+1], [B, T+2]..so it continues the generation in all the batch dimensions in the time dimension, and it will do this for `max_new_tokens`.\n",
        "  * whatever is predicted in the `for` loop is concatenated on top of the previous `idx` along the first dimension which is the time dimension, so that becomes the new `idx`\n",
        "  * inside the `for` loop for `generate`:\n",
        "    * we're taking the current indices `idx`, we're getting the predictions inside the `logits`, and then the loss will be ignored over there because we don't have any truth targets that we're going to be comparing with\n",
        "    * once we get the `logits`, we are only focusing on the last step, so instead of [B, T, C], we're going to pluck out the -1 (the last element in the time dimension) because those are the predictions for what comes next and so that gives us the `logits` which we then convert to `probs` via `softmax`. Then we use `torch.multinomial` to sample from those `probs` and we ask pytorch to gives us 1 sample\n",
        "    * `idx_next` will then become [B,1] because in each of the batch dimensions, we're going to have a single prediction for what comes next. The `num_samples = 1` will make the [B, T] be a [B, 1]\n",
        "    * we're going to take those integers `idx_next` that come from the sampling process according to the probability distribution given here and those integers got just concatenated on top of the current sort of running stream of integers `idx` and this gives us [B, T+1]\n",
        "    * we're calling `self(idx)` which will end up going to the `forward` function. `Targets=None` thus has to be optional as we haven't provided any, and as a result there is no `loss` to create, so we just get the `logits` when there are no `targets`. If there are `targets` it just passes them and gives us `loss`.\n",
        "    * the `generate` function is written to be general but it's kind of ridiculous right now because we're feeding in all this stuff, we're building out the context and concatenating it all, and we're always feeding it into the model. but it's ridiculous because it is a simple bigram model so in order to make a prediction about `k` for example, we only need a `w`, but what we fed into the model is that we fed the entire sequence and then we only looked at the last pieces and predicted `k`. We're writing the `generate` function in this way right now because it's a bigram model but i'd like to keep this function fixed and i'd like it to work later when our character further look in the history. Right now the history is not used so it looks silly, but eventually the history will be used so we do it this way.\n",
        "\n",
        "* decoding this:\n",
        "  ```python\n",
        "  print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
        "  ```\n",
        "  * the `idx` are `torch.zeros((1, 1), dtype=torch.long`.\n",
        "    * we're creating a `batch` which will just be 1, we're creating a [1,1] tensor and it's holding a 0 and the d.type is int. 0 (representing new line character) is how we're going to kick off generation\n",
        "  * then we will ask for 100 tokens and then enter generate and continue that.\n",
        "    * since `generate` works on the level of `batches`, we then have to index into the 0th row to basically unplug the single batch dimension that exists and then that gives us a time steps which is just a 1D array of all indices which we will convert to simple python list form pytorch tensor so that can feed into our `decode` function and convert those integers to text\n",
        "  * generates total garbage because it's a totally random model so next we'll train this model\n",
        "\n"
      ],
      "metadata": {
        "id": "Wdp53hXID-VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH_KaHX_GoBS",
        "outputId": "f3381ba8-ccee-4ca2-a1bf-df55e309a79e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Training the model\n",
        "\n",
        "* `Adam` is a much more advanced optimizer than `SGD`. it works extremely well for a typical good setting for the lr to be 1e-3, but for small models you can use a much higher `lr`\n",
        "  * optimizers take the gradients and update the parameters using the gradients\n",
        "* this is a very simple model because the tokens are not talking to each other so given the previous context of whatever was generated, we're just looking at the last character to make the predictions about what comes next.\n",
        "* the tokens now have to start talking to each other and figure out what is in the `context` -> kicking off the Transformer"
      ],
      "metadata": {
        "id": "kuMJTc8rG-VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "ooiKNPb8R_vN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(1000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss through a training loop\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXmsAt3HSAAx",
        "outputId": "634747d1-ef4d-4bdc-9fb4-c6f773d9a9f8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7218432426452637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# something more reasonable (can increase number of tokens to check results)\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iq_EaUVySBq4",
        "outputId": "8bfafa8a-54e7-49bc-b972-5e49e0ae2184"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "olylvLLko'TMyatyIoconxad.?-tNSqYPsx&bF.oiR;BD$dZBMZv'K f bRSmIKptRPly:AUC&$zLK,qUEy&Ay;ZxjKVhmrdagC-bTop-QJe.H?x\n",
            "JGF&pwst-P sti.hlEsu;w:w a BG:tLhMk,epdhlay'sVzLq--ERwXUzDnq-bn czXxxI&V&Pynnl,s,Ioto!uvixwC-IJXElrgm C-.bcoCPJ\n",
            "IMphsevhO AL!-K:AIkpre,\n",
            "rPHEJUzV;P?uN3b?ohoRiBUENoV3B&jumNL;Aik,\n",
            "xf -IEKROn JSyYWW?n 'ay;:weO'AqVzPyoiBL? seAX3Dot,iy.xyIcf r!!ul-Koi:x pZrAQly'v'a;vEzN\n",
            "BwowKo'MBqF$PPFb\n",
            "CjYX3beT,lZ qdda!wfgmJP\n",
            "DUfNXmnQU mvcv?nlnQF$JUAAywNocd  bGSPyAlprNeQnq-GRSVUP.Ja!IBoDqfI&xJM AXEHV&DKvRS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we take our code in this jupyter notebook and simplify our immediate work into just the final product we have at this point into `bigram.py` (our starter code):\n",
        "  * at the top we just set up the hyperparameters that we've defined\n",
        "  * reproducibility\n",
        "  * read the data\n",
        "  * get the encoder and the decoder\n",
        "  * create the training test splits\n",
        "  * use the dataloader that gets a batch of the `inputs` and `targets`\n",
        "  * estimate loss (talkeed later ahead)\n",
        "  * `BigramLanguageModel` whuch can forward and give us logits and loss and it can generate\n",
        "  * optimizer + training loop\n",
        "\n",
        "things added not discussed from before:\n",
        "  * added `device` to allow it to be run on a GPU (Cuda) instead of CPU\n",
        "    * we need device agnostic code to move the data, the model (its parameters) to the device\n",
        "      * e.g `nn.Embedding` table and it has got a `.weight` inside it which stores the lookup table\n",
        "  * inside the training loop we introduce from the `estimate_loss` function:\n",
        "  ```python\n",
        "  print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "  ```\n",
        "    * before we were just printing `loss.item()` outside the loop which is a very noisy measurement of the current loss as every batch will be more or less lucky\n",
        "    * it calls the `estimate_loss` function which averages the loss over multiple batches for both splits which will be much less noisy\n",
        "  * in the `estimate_loss` function:  \n",
        "    * first we put the model into evaluation mode before it averages the loss and then into training mode after it is done averaging loss. For our model right now it won't do anything because we just have the `nn.Embedding` layer; we have no Dropout or BatchNorm layers but it is still a good practice to think through which mode your model is in since some layers will have different behaviour at `inference` time or `training` time\n",
        "    * there is also a context manager `@torch.no_grad()` which is just telling pytorch that whatver happens inside the function, we will not call `.backward()` on and so pytorch can be very efficient in its memory use as it will no longer have to store all the intermediate variables."
      ],
      "metadata": {
        "id": "_X8p8lU4UjH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Mathematical tricks in self-attention"
      ],
      "metadata": {
        "id": "kFEc7r4XT0rB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 version 1: averaging past context with for loop, weakest form of aggregation\n",
        "\n",
        "* we would like the 8 tokens in the toy example to talk to each other by coupling them\n",
        "  * in particular, we want to couple them in a very specific way. The token at the 5th location should not communicate with tokens in the 6th, 7th or 8th location because those are future tokens in the sequence. It should only talk to the 1st 2nd 3rd 4th token. So, the information only flows from previous context to the current timestamp, and we cannot get any information from the future because we are about to try to predict the future\n",
        "  * what is the easiest way for the tokens to communicate?\n",
        "    * if we are the 5th token and i want to communicate with my past, the simplest way to do is just an average of all the preceding elements. As the 5th token, i would like to take the channels that make up the information at my step, but also the channels from the previous steps, and then average them, and that will become the feature vector that kind of summarizes the 5th token in the context of history.\n",
        "    * doing a `sum()` or an average is an extremely weak sort of interaction, like the arrangment is extremely lossy. We've lost information about the spatial arrangements of all those tokens but that's okay for now, we'll bring in that information back later.\n",
        "* for now what we would like to do is for every single batch independently, for every teeth token in that sequence, we would like to now calculate the average of all the vectors in all the previous tokens and also at this specific token:\n",
        "  * we will create `xbow` where `bow` is short for backup words because it is like a term people use when averaging things -> basically there's a word stored at each one of these 8 locations and we're just averaging\n",
        "  * we initialize at 0 and we do a `for` loop (not efficient yet) to iterate over all the batch dimensions independently iterating over time and then the previous tokens are at that specific batch dimension and then everything up to and including the teeth token. So, when we slice out `X` in this way, `xprev` becomes of shape [teeth_tokens(t), C] -> previous chunk of tokens from my current sequence. Finally, we do an average or `mean` for the time over the 0th dimension -> we will get a small C 1D vector which we will store in `xbow`.\n",
        "  * the last token will be an average of all the tokens with them being added vertically\n",
        "\n",
        "* we can make this very efficient using a mathematical trick: matrix multiplication\n"
      ],
      "metadata": {
        "id": "mWRNM5lOfo11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cl6skYhf2Fw",
        "outputId": "37771ea8-24e5-47e0-ac21-dd89dfa4fe44"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "9NTxipNzhbYV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 using matrix multiplication to make it efficient\n",
        "\n"
      ],
      "metadata": {
        "id": "zS1shEkbi8ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* the number in the top left of matrix `c` (14 for us), is achieved by the first row of `a` dot product with the first column of `b`..and so this continues as a matrix multiplication in the form of a dot product to get `c` -> using `torch.ones`\n"
      ],
      "metadata": {
        "id": "SZxj6y-YkExS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initial toy example with matrix multiplication\n",
        "torch.manual_seed(42)\n",
        "a = torch.ones(3, 3)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-0xg4Zifu5V",
        "outputId": "182608ec-95cd-4728-abc7-34b65977f0ea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[14., 16.],\n",
            "        [14., 16.],\n",
            "        [14., 16.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* the trick here is:\n",
        "  * to use `torch.tril` and then wrap it in `torch.ones` and it will just return the lower triangular matrix as one's -> changes all the matrices hence\n",
        "    * depending on how many ones and zeros we have here, what we are basically doing here is a `sum` currently of a variable number of the rows of `a` and `b` and that gets deposited into `c`. We're doing sums cuz they are one's in `a`\n",
        "  * on the alternative, we can also do an average in an incremental fashion because we can basically normalize the rows so that they sum to one and then we're going to get an average\n",
        "    * here now our matrix `a` has rows which are normalized (summing upto 1). Using matrix `b`, the matrix `c` first row is just the first row of `b` itself, the second row of `c` is the average of the first 2 rows of `b` (column wise addition). in the last row of `c` we are getting an average of the 3 rows (column wise addition)\n",
        "* by manipulating these elements of this multiplying matrix and then multipplying it with any given matrix, we can do these averages in these incremental fashion, and we can just manipulate that on basis of `a`"
      ],
      "metadata": {
        "id": "MdMBR4bfkKEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew7w3Xi8fwXy",
        "outputId": "1661e600-99aa-47d3-8e59-e0b5a2a8d07b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Version 2: vectorizing it with matrix multiply for more efficiency\n",
        "\n",
        "* `wei` is how much of every row we want to average up and it's going to be an average as the rows sum upto 1. Hence, this is going to be our `a`\n",
        "  * our `b` in this example is `x`\n",
        "* pytorch will come in `xbow2` and see that (T, T) @ (B, T, C) shapes are not the same, so it will create a batch dimension here and it becomes this: (B, T, T) @ (B, T, C) leading to (B, T, C)\n",
        "  * basically it wil apply the matrix multiplication in all of the batch elements in parallel and individually, and for each batch element there will be a (T,T) multoplying (T,C)\n",
        "* we were able to use batch multiply to do this aggregation and the weights are specified in the (T,T) array and we're basically doing weighted sums, these weighted sums are according to the weights inside:\n",
        "```python\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "```\n",
        "  * they take on off a sort of triangular form and so that means that the token at the teeth dimension will only get tokens information from the tokens preceding it."
      ],
      "metadata": {
        "id": "T4bx-vI5kjjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# xbow"
      ],
      "metadata": {
        "id": "tg1fwY0HoEhO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xbow2"
      ],
      "metadata": {
        "id": "uMnMmvDXoFs2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2, rtol=1e-05, atol=1e-07) # using this tolerance because of floating point differences in the numbers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cmc1i7r5mhW7",
        "outputId": "3033f42a-735f-4203-ce32-51f4d66d00fa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Version 3: adding `softmax`\n",
        "\n",
        "* identical to the 1st and 2nd version\n",
        "* `trils` begins with all 0. So if I just print the beginning of it, it's all 0. Then I use the masked fill. So what this is doing is, going down the masked fill, it's all 0s. For all the elements where `tril` is equal to 0, make them a negative infinity.\n",
        "\n",
        "* Then the final one here is the `softmax`. So if I take the softmax along every single row, what is that going to do? Well, the softmax is also like a normalization equation, right? And so it's all over here, and we get the exact same matrix. We'll bring back the softmax. And recall that in softmax, we're going to exponentiate every single row once, and then we're going to divide by itself. And so if we exponentiate every single element here, we're going to get 1, and here we're going to get 0, 0, 0, 0, 0, 0, 0. And then when we normalize, here we're going to get 1, 0, and then 0s. And the softmax will again divide, and this opens up like that, and so on. And so this is also the same way to produce the masked fill.\n",
        "\n",
        "* Now the reason that this is a bit more interesting, and the reason that we're going to end up using it in self-attachment, is that these weights here begin with 0. And you can think of this as like an interaction string, or like an affinity. So basically, it's telling us how much of each token from the past we want to aggregate, and average over. And then this line is saying, tokens from the past cannot communicate.\n",
        "\n",
        "* By setting them to negative infinity, we're saying that we will not aggregate anything from those tokens. And so basically, as that goes for softmax, this is the aggregation through matrix multiplication. And so what this is now is, you can think of these as, these 0s are currently just set by us to be 0, but a quick preview is that these affinities between the tokens are not going to be just constant at 0. They're going to be data-dependent.\n",
        "\n",
        "* These tokens are going to start looking at each other, and some tokens will find other tokens more or less interesting. And depending on what their values are, they're going to find each other interesting to different amounts, and are going to have those affinities. And then here we are saying, the future cannot communicate with the past.We're going to collect them. And then when we normalize and sum, we're going to aggregate some of their values, depending on how interesting they find each other. And so that's the preview for softmax.\n",
        "\n",
        "* And basically, long story short from this entire section is that, you can do weighted aggregations of your past elements by using matrix multiplication of a lower-triangular function. And then the elements here in the lower-triangular part are telling you how much of each element fuses into this position. So we're going to use this technique now to enable self-attention with them.\n"
      ],
      "metadata": {
        "id": "1lJFVcyvmh35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T)) # lower triangular matrix of one's\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "id": "NvWk1VrRqaZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1865a317-40a4-45dc-e400-e4f39b16ad47"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "check `gpt.py` for this code:\n",
        "* more than just encoding the identity of the tokens, we also token their position. Hence, we introduce a second embedding table as `self.position_embedding_table` which is an embedding of block size by an embedding table, so each position from `0 to block_size - 1` will also get its own embedding vector.\n",
        "  * we will also have `pos_emb` which will just be basically integers from 0 to t-1, and all of those integers get embedded from the table to create a [T,C] and then logits just take `x`, where `x` is the addition of the `token_emb` and the `pos_emb`, and hence the broadcasting note will then work out -> [B, T, C] + [T, C] gets right aligned and a new dimension of 1 gets added and it gets broadcasted across the batch. So, `x` at this point not only holds the token identities but the positions these tokens occur. This is not that useful because we just have a simple `bigram` model right now so it doesn't matter if we are at the 5th position, it's all translation invariant at this stage so this information currently won't help but as we work on the self-attention block, we'll see that it matters."
      ],
      "metadata": {
        "id": "LFhRdiPg9G_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 The Crux: Version 4: self-attention\n",
        "\n",
        "* the code we had before just does a simple weight and a simple average of all the past tokens and the current token (the previous and current information is just being mixed in an average) -> our initial below code does the same by creating a lower triangular structure which allows us to mask out the `wei` matrix we create and then we normalize it, and currently when we initialize all the affinities between all the different sort of tokens or nodes to be 0, then we see that `wei` gives us this structure where every single row has uniform numbers.\n",
        "  * we don't want all of it to be uniform because different tokens will find different other tokens more or less interesting, and we want that to be data dependent when trying to get information from the past\n",
        "* what self-attention does:\n",
        "  * every single node or single token at each position will emit two vectors, it will emit a query and a key\n",
        "  * the query vector: \"what am i looking for\"\n",
        "  * the key vector: \"what do i contain\"\n",
        "  * the way we get affinities between these tokens now in a sequence is we basically just do a dot product between the keys and the queries to get `wei`\n",
        "    * if the key and query are aligned, they will interact a very high amount and then we will get to learn more about that specific token than any other token in the sequence\n",
        "  * when we forward the lineary layer on top of our `x`, all the tokens in all the positions in the [B, T] arrangement -> all of them in parallel and independently produce a `key` and `query` without any communication happening yet\n",
        "    * when the communication takes place, all the `queries` will dot product with all the `keys`. Hence, we want the affinities or `wei` between these to be query mutltiplying `key` -> for this we need to transpose the last 2 dimensions of `key` to get (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "      * for every row of B, we will now have a T^2 matrix giving us the affinities\n",
        "  * first `wei` was applied the same to all of the batch elements, but now every single batch element will have different sort of `wei` because every single batch elements contains different tokens at differen positions so now it is data dependent and not uniform\n",
        "    * for e.g: the 8th row knows what content it has and at what position it's in. Now the 8th token based on that creates a `query` (e.g: \"i'm a vowel, i'm at the 8th position and i'm looking for any consonants at positions up to 4\". Then all the nodes get to emit the `keys` and maybe one of the channels will be \"i'm a consonant and in a position upto 4\", and that `key` will have a high number in that specific channel and that's how when the `key` and `query` dot product they can find each other and create a high affinity -> when they have a high affinfity, then through `softmax` we will end up aggregating a lot of its information into my position, so i'll get to learn a lot about it) -> we now get a nice distributuin that sums to 1. This is now telling us in a data dependent manner how much of information to aggregate from these tokens in the past\n",
        "  * another part to self-attention head: when we do aggregation, we don't aggregate the tokens exactly, we produce one more value and we call that `value` and instead of matrix multiplying `wei` with `x`, we just calculate `v` which is achieved by propogating the linear layer on top of `x` again and then we output `wei @ v`. Hence, `v` is the elements that we aggregate or the vector we aggregate instead of the raw `x` making the output of the single head as 16D as that is `head_size`.\n",
        "  * So, we can think of `x` as kind of like private information to the token"
      ],
      "metadata": {
        "id": "9i4W-WBxqgdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "# wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1) # exponentiate and normalize the negative values\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "# out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1C4kKWx-vdI",
        "outputId": "477d90f5-c4ec-49e7-f7e9-74d6cb99d017"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWxqzVROG2aP",
        "outputId": "66ac466f-0e1b-435a-b0f9-d022f353662a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes:**\n",
        "\n",
        "1.  Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "\n",
        "  <img src = \"https://raw.githubusercontent.com/Raahim58/Neural-networks/main/images/self-attention.png\" width = 300 height = 300>\n",
        "\n",
        "  * our graph doesn't look like this:\n",
        "    * we have 8 nodes because our `block_size` is 8, and hence there are always 8 tokens.\n",
        "    * the first node is only pointed to by itself, and the second node is pointed to by the first node and by itself all the way upto 8th node which is pointed to by all the previous nodes and by itself\n",
        "    * attention can be applied to any arbitrary directed graph and it's just a communication mechansim between the nodes.\n",
        "\n",
        "2. There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "  * by default these nodes have no idea where they are positioned in space and that's why we need to encode them positionally and give the some information that is anchored to a specific position so that they know where they are\n",
        "  * that is different from convolution, as in convolution there is a very specific layout of information in space and the convolutional filters act in space.\n",
        "  * in attention, there is just a set of vectors out in space, they communicate, and if you want them to have a notion of space you need to specifically add it, which is what we did when we calculated the relative position code encodings and added that information to the vectors\n",
        "\n",
        "3. Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "  * since the `batch_size` is 4, we have 4 seperate pools of 8 nodes and those eight nodes only talk to each other, but in total they're like 32 nodes that are being processed but there's basically 4 pools of 8\n",
        "\n",
        "4. In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "  * in the attention block, we have a specific structure of directed graph where the future tokens will not communicate to the past tokens, but this doesn't necessarily have to be the constraint in the general case. In fact, in many cases we might want to have the nodes talk to each other fully (e.g in sentiment analysis where they might be a lot of number of tokens, and we may want to have them all talk to each other fully because later you are predicting the sentiment of the sentence and so it's okay for the nodes to talk to each other\n",
        "  * in those cases, we will use an encoder block of self-attention and all it means that it's an encoder block is that we will delete:\n",
        "  ```python\n",
        "  wei = wei.masked_fill(tril == 0, float('-inf')) # deleting will allow all the nodes to completely to talk to each other (in encoder)\n",
        "  ```\n",
        "  * what we're implementing above is usually called the decoder block and it's called the decoder because it's sort of like a decoding language and it's got an auto regressive format where one has to mask with the triangular matrix so that nodes from the future never talk to the past because that would give away the answer. So in the encoder block you delete the above line of code allowing all the nodes to talk to each other, and in the decoder block it stays so we have the triangular structure\n",
        "    * attention, however, doesn't care. Attention supports arbitrary connectivity between nodes.\n",
        "\n",
        "5. \"self-attention\" just means that the keys and values are produced from the same source (`x`) as queries. In \"cross-attention\", the queries still get produced from `x`, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "  * attention is more general than that. In encoder & decoder transformers, for example, you can have a case where queries are produced from `x` but the `keys` and `queries` come from a whole seperate external source and sometimes from the the encoder blocks that encode some context that we'd like to condition on. So, the `keys` and the `values` will actually come from a whole seperate source which are nodes on the side and we're just producing `queries` and we're reading off information from the side so `cross-attention` is used when there's a seperate source of nodes we're like to pool information from into our nodes.\n",
        "\n",
        "6. \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, `wei` will be unit variance too and Softmax will stay diffuse and not saturate too much.\n",
        "  * it's an important normalization to have. The problem is if you have unit gaussian inputs and if you do `wei` naively, then you see that the `var` willl be on the order of `head_size` which in our case is 16, but if you multiply that by 1/sqrt(head_size), then the `variance` will be 1 hence being preserved\n",
        "  * this is important because `wei` feeds into `softmax` so it's really important especially at initialization, that `wei` be fairly diffused\n",
        "    * the problem is because of `softmax`, the `wei` takes on very positive and very negative numbers inside it. `softmax` will actually converge towards one-hot vectors\n",
        "    * if we start sharpening the numbers and making them bigger by multiplying the numbers by 8, the `softmax` will start to sharpen, and it will sharpen towards the max so it will shapen toward whatever number in the `softmax` feeding in is the highest. We don't want these values at intialization to be too extreme otherwise `softmax` will be way too peaky and you're basically aggregating the information from a single node. Every node just aggregates information from a single other node which is not what we want, especially at initialization. So, the scaling is used just to control the variance at initialization.\n",
        "\n",
        "\n",
        "Illustration below"
      ],
      "metadata": {
        "id": "hSeGybQRGr8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "S3pZIrrjA6z0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var(), q.var(), wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiv3lhjQROeB",
        "outputId": "84609c85-b108-4a85-8472-d5ff5a46c20f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.0449), tensor(1.0700), tensor(1.0918))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ-_eyRERS4O",
        "outputId": "3d6cca36-b0f5-4a44-d260-36e040cdb8c1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEuUtHffRU3h",
        "outputId": "476f8925-8e1b-4fb2-a014-a76b71218892"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Addings more features"
      ],
      "metadata": {
        "id": "LcUF_vEySyXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Scaled product dot attention"
      ],
      "metadata": {
        "id": "8U2BZYb9S1Lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* making one more change in the `head` block:\n",
        "  * in `generate`, we have to make sure that the `idx` that we feed into the model, since we're using positional embeddings, we can never have more than the `block_size` coming in because if `idx` is more than the `block_size`, then our position embedding table will run out of scoprt because it only had embeddings for up to `block_size`, and so therefore we crop the context that we're going to feed into self so that we never pass in more than `block_size` elements\n",
        "  * we also decrease the `lr` because the self-attention can't tolerate very high `lr`\n",
        "  * we also increase the number of iterations because the `lr` is lower and then we train it. Previously, we got the `loss` as **2.5** and now we are down to **2.4**\n",
        "\n",
        "      <img src = \"https://raw.githubusercontent.com/Raahim58/Neural-networks/main/images/scaled%20product%20attention.png\" width = 600 height = 400>"
      ],
      "metadata": {
        "id": "FNyWrTElRyQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "WSDbWgYEZK1O"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Multi-head attention"
      ],
      "metadata": {
        "id": "4aOljd6nRXpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* it's applying multiple heads in attention and concatenating the results\n",
        "\n",
        "  <img src = \"https://raw.githubusercontent.com/Raahim58/Neural-networks/main/images/multi%20head%20attention.png\" width = 400 height = 400>\n",
        "\n",
        "* how many heads do we want? and what's the size for each head? Then we run all of them in parallel in a list and simply concatenate the outputs over the channel dimension\n",
        "* instead of having one communication channel, we now have 4 communication channels in parallel and each of these communication channels typically will be smaller correspondingly -> because, we have 4 communication channels, we want 8D self-attention. So, from each communication channel we're going to gather 8D vectors and then we have 4 of them and that concatenates to give us 32 which is the original `n_embd`\n",
        "  * this like a group convolution because basically instead of having a one large convolution, we do convolutional groups and that's multi-headed self-attention\n",
        "  * it helps to create multiple independent channels of communication, gather lots of different types of data, and then decode the output"
      ],
      "metadata": {
        "id": "wCUIlnCVS8CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "NHV4lgdoZGID"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 Feed forward layer"
      ],
      "metadata": {
        "id": "Bki36-cDV4_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* it's a layer consisting of a linear layer followed by a ReLU non-linearity\n",
        "* `feed forward` or `ffwd` is called sequentially right after the self-attention so we self-attend, then we feed forward.\n",
        "* the feed forward when it's applying linear, it's doing it on a per token level. All the tokens do this independently so the self-attention is the communication and then once they've gathered all the data, now they need to think on that data individually\n",
        "* the validation loss continues to go low going down from **2.28** to **2.24**\n",
        "* we will not intersperse the communication with the computation and that's also what the transformer does when it has blocks that communicate and then compute, and it groups them and replicates them"
      ],
      "metadata": {
        "id": "TuOh3QXlV-cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd, n_embd)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "mw5oFxnRZCqT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Residual connections"
      ],
      "metadata": {
        "id": "1Km24Py9aIb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://raw.githubusercontent.com/Raahim58/Neural-networks/main/images/residual%20connection.png\" width = 400 height = 400>\n",
        "\n",
        "* the block intersperses communication and computation\n",
        "* the communication is done using multi headed self-attention and the computation is done using the feed-forward network on all the tokens independently\n",
        "* `n_head` is like the group size in group convolution\n",
        "* since `n_embd` is 32, the `n_head` is 4 and the `head_size` should be 8 so that everything works out channel wise\n",
        "* when we try to run the code with `Block`, we don't end up getting a very good answer. The reason for that is that we're building a very deep neural net, and deep neural nets suffer from optimization issues\n",
        "* 2 optimizations that make the transformer optimizable:\n",
        "  1. residual connections: arrows that skip from one block to the `add & norm` layer which comes from the 2015 paper **Deep Residual learning for Image recognition**\n",
        "    * what this means is that you transform data but then you have a skip connection with addition from the previous features\n",
        "    * there is a computation from top to bottom consisting of a residual pathway in which you are free to fork off to perform some computation and then project back to residual pathway via addition. So, you go from the inputs to targets via only plus and plus\n",
        "      * this is useful because, recall in backpropogation from micrograd video -> addition distributes gradients equally to both of its branches that is as fat as the input, so the supervision or the gradients from the loss hop through every addition node all the way to the input\n",
        "      * they then also fork off into the residual blocks but basically you have this gradient super highway that goes directly from the supervision all the way to the input unimpeded. These residual blocks are usually initialized so they contribute very very. little if anything to the residual pathway. They are initialized that way so in the beginning they are kind of not there, but during the optimization, they (`block`) come online over time and they start to contribute.\n",
        "    * we introduce `projection` in the `multiheadattention block`, which is just a linear transformation of the outcome of the `forward` layer in the `multiheadattention block`. So that's the projection back into the residual pathway\n",
        "    ```python\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    ```\n",
        "    * in the paper the dimensionality for the model is 512 and the dimensionality for the feed-forward layer is 2048 so a multiplier of 4. So, the inner layer of the feed-forwar network should be multiplied by 4 in terms of channel sizes. We add more computation to the block on the side of the residual pathway\n",
        "    * we trained it to get a val loss of **2.08** and the network is getting bigger, so the train loss is gettinng ahead of validation loss so we're seeing a bit of overfitting -> generations still not great but getting close to English."
      ],
      "metadata": {
        "id": "coS_A3FeaVB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(x) # sa is self attention, forking off for communication and coming back\n",
        "        x = x + self.ffwd(x) # ffwd is feed forward, forking off for computation and coming back\n",
        "        return x"
      ],
      "metadata": {
        "id": "MFfuKvJcaNw4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "qgkPwpkQgK6W"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5 LayerNorm"
      ],
      "metadata": {
        "id": "-mRRShZglvG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* implemented in pyTorch based on a paper\n",
        "* very similar to BatchNorm whuch just made sure that across the batch dimension, any individual neuron had unit gaussian distribution (0 mean and 1 std)\n",
        "  * batchNorm guarantees that when we look at just the 0th column, it's a 0 mean 1 std -> normalizing every single column of this input, but the rows won't be initialized by default\n",
        "* we normalize the rows instead of the columns in LayerNorm. Since our computation does not span across examples, we can delete all the buffers since we can always apply the operation.\n",
        "  * there is no distinction between training and test time. We do keep gamma and beta but we don't need momentum, we don't care if it's training or not.\n",
        "* deviating from the original paper:\n",
        "  * it is now common to apply the LayerNorm before the transformation, instead of after the transformation like in the paper -> pre-Norm Layer implementation"
      ],
      "metadata": {
        "id": "JhU0GYrVlzJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6FpqJd_mHl4",
        "outputId": "4b828bcb-232e-4159-a874-dd4be81c92ab"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnwTa_thmM5t",
        "outputId": "20d7b961-b49b-43fe-b0fd-6253a0865c8c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyT2sPyDmO_3",
        "outputId": "defbdc09-1570-4f55-f1ee-86392b2435fb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  * when layerNorm is applying the normalization features in `Block`, the mean and the variance are taken over 32 numbers so the batch and time act as batch dimension so this is kind of like a per token transformation that just normalizes the features and makes them unit gaussian at initialization\n",
        "  * these LayerNorms have gamma and beta trainable parameters inside them, the LayerNorm will eventually create outputs that might not be unit gaussian but the optimization will determine that\n",
        "  * get down to **2.06** from **2.08** -> will help more with deeper neural nets\n",
        "  * a layernorm also added at the end of the transformer and right before the final linear layer that decodes into the vocabulary"
      ],
      "metadata": {
        "id": "eiih8iSJoQo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.6 Scaling up + adding dropout"
      ],
      "metadata": {
        "id": "-0wUtbr-oDft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* introduced `n_layer` which just specifies how many layers of the blocks we're going to have + a new variable `n_heads` in Bigram Language Model\n",
        "* introduced `Dropout`:\n",
        "  * it is something that you can add right before the residual connection back into the original pathway\n",
        "  * this helps us stop some of the nodes from randomly communicating\n",
        "  * it comes from a 2014 paper: **Dropout: A simple way to prevent neural networks from overfitting**\n",
        "  * basically it takes your neural net, and it randomly every forward and backward pass shuts off some subset of neurons -> randomly drops them to 0 and trains without them and what this does is, because the mask of being dropped out has changed every single forward backward pass, it ends up training an ensemble of networks and at test time everything is fully enabled and kind of all of those sub networks are merged into a single Ensemble\n",
        "  * regularization technique and used mainly when scaling models to stop overfitting"
      ],
      "metadata": {
        "id": "sGhVk8U7p3Ul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Encoder vs Decoder"
      ],
      "metadata": {
        "id": "miFG3Pzmuld0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n"
      ],
      "metadata": {
        "id": "EIb2VLQDusRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Decoder-Only Transformer Architecture:**\n",
        "\n",
        "* The model implemented is a decoder-only transformer.\n",
        "* There is no encoder component in this architecture.\n",
        "* The architecture also lacks a cross-attention block.\n",
        "* The block only contains a self-attention mechanism and a decoder.\n",
        "* The model is missing the cross-attention piece that is usually present between the encoder and decoder in a traditional transformer.\n",
        "\n",
        "**2. Explanation of the Decoder:**\n",
        "\n",
        "* A decoder-only model is used in this implementation because the model is focused solely on text generation.\n",
        "* The generation process is unconditioned, meaning it operates without external input or constraints, just \"blabbering on\" based on guesses.\n",
        "* The model uses a triangular mask in its transformer architecture.\n",
        "This mask gives the model its autoregressive property, allowing it to generate text token-by-token.\n",
        "* The triangular mask prevents future tokens from being attended to during the generation process, ensuring that predictions are made based only on the past tokens.\n",
        "\n",
        "\n",
        "**3. Difference Between Encoder-Decoder and Decoder-Only:**\n",
        "\n",
        "* The original paper on transformers presented an encoder-decoder architecture for machine translation.\n",
        "* In that context, the model is designed to translate from one language (e.g., French) to another (e.g., English).\n",
        "* The encoder processes the input sentence (in French), while the decoder generates the translated sentence (in English).\n",
        "* Typically, the process starts with special tokens that guide the translation process:\n",
        "* The model reads and conditions on the input tokens (French sentence).\n",
        "* A special token is introduced at the beginning to start the generation process.\n",
        "* The model then generates output tokens (English sentence) and concludes with an end token.\n",
        "* The actual generation process in both the encoder-decoder and the decoder-only models is the same, but the encoder-decoder model is conditioned on additional input information.\n",
        "\n",
        "**4. Working of the Encoder-Decoder Model:**\n",
        "\n",
        "* In a machine translation model, the encoder processes the French sentence and creates token representations from it.\n",
        "* The transformer processes these tokens without the triangular mask, allowing the tokens to attend to each other freely.\n",
        "* After encoding the French sentence, the model generates an output from the decoder.\n",
        "* The decoder not only relies on past tokens for prediction but also takes in the fully encoded French prompt through cross-attention.\n",
        "* Cross-attention integrates information from the encoder (French sentence) with the current state of the decoder (English sentence generation).\n",
        "\n",
        "**5. Why the Implemented Model Doesn’t Use an Encoder:**\n",
        "\n",
        "* The presented model does not include an encoder because there’s nothing to encode – it doesn’t require conditioning on any additional input.\n",
        "* The model's purpose is to imitate text generation from a given text file, making the encoder unnecessary.\n",
        "This decoder-only transformer architecture is similar to the one used in GPT (Generative Pretrained Transformer).\n",
        "\n",
        "This structure explains the workings of both decoder-only and encoder-decoder transformers and the specific choice of architecture for text generation tasks."
      ],
      "metadata": {
        "id": "7Tod27rAuo2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Full training code (Bigram Model)"
      ],
      "metadata": {
        "id": "k74DaI8maF_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n",
        "# Writing the generated text to 'more.txt'\n",
        "open('output.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlDO6urdUAJl",
        "outputId": "6fb3bfd0-b4a8-4eef-8099-2d11b5431dc4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n",
            "step 0: train loss 4.2849, val loss 4.2823\n",
            "step 500: train loss 2.0112, val loss 2.0971\n",
            "step 1000: train loss 1.6021, val loss 1.7830\n",
            "step 1500: train loss 1.4412, val loss 1.6396\n",
            "step 2000: train loss 1.3430, val loss 1.5724\n",
            "step 2500: train loss 1.2809, val loss 1.5330\n",
            "step 3000: train loss 1.2268, val loss 1.5094\n",
            "step 3500: train loss 1.1824, val loss 1.4881\n",
            "step 4000: train loss 1.1475, val loss 1.4869\n",
            "step 4500: train loss 1.1108, val loss 1.4805\n",
            "step 4999: train loss 1.0779, val loss 1.4920\n",
            "\n",
            "But with prison: I will stead with you.\n",
            "\n",
            "ISABELLA:\n",
            "Carress, all do; and I'll say your honour self good:\n",
            "Then I'll regn your highness and\n",
            "Compell'd by my sweet gates that you may:\n",
            "Valiant make how I heard of you.\n",
            "\n",
            "ANGELO:\n",
            "Nay, sir, Isay!\n",
            "\n",
            "ISABELLA:\n",
            "I am sweet men sister as you steed.\n",
            "\n",
            "LUCIO:\n",
            "As it if you in the case would princily,\n",
            "I'll rote, sir, I did cannot now at me?\n",
            "That look thence, thy children shall be you called.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Marry, though I do read you!\n",
            "\n",
            "LUCIO:\n",
            "O that mufflest than that should do worse a mode,\n",
            "By good clopHelden brick, your petite infect,\n",
            "Give mattering summour; I pray you have an eanning of you,\n",
            "May be past a press'd, so we show with my walls.\n",
            "I slept, I play; for I am, but will.\n",
            "\n",
            "Second Peter:\n",
            "Hold Claudio you that sees to meet you, Tranio;\n",
            "Her well with my wounds shall see ht; but he were\n",
            "a smoth way his eweary wanto-mou rich on our\n",
            "A rose faitter gash; parce mo know that he did.\n",
            "\n",
            "DUCHESS OF YORY:\n",
            "Why, how far, ay? see, methought be not upon't?\n",
            "\n",
            "RICHMOND:\n",
            "No more!\n",
            "Beseech you are I turn the banish\n",
            "Nothing removed and turn'd the king sight.\n",
            "\n",
            "RICHARD:\n",
            "It mean us grows muture with now, like some tailor,\n",
            "That gliers make him speeding leaves ranted this law,\n",
            "And may as the noble liberty hatch,\n",
            "Something me high all buy, as well her she carried\n",
            "As young my demands? is clear?\n",
            "Love in my loyal pleggage?\n",
            "\n",
            "LEONTES:\n",
            "Fair God's fordship's noble could. Pray, sir, for\n",
            "my weeds, stips: old the wisestray follow.\n",
            "\n",
            "WELBOW:\n",
            "Pray, you are not honour.\n",
            "\n",
            "POLIXENES:\n",
            "My lords leve, you'll not so be. give me leave\n",
            "your father, for the offence but set this taughting\n",
            "war the su glass. Here in blood me with oath.\n",
            "What jot she'll cont many in meny fool\n",
            "Have yielege it, I acconds say 'tis beyn, but\n",
            "Loopless it dark,\n",
            "the plantage: would I, as we\n",
            "Hath thy son a word, year my husband; who\n",
            "shall meet there comet; and, even out the noble warter,\n",
            "As the whore our grace would slave, so much,\n",
            "He chides me loves me in for usurpians,\n",
            "Which the beggars he with no presently t\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10001"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('output.txt', 'r') as f:\n",
        "    print(f.read())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFHA69MZURVF",
        "outputId": "1c4574bc-9e7b-49f6-9ea9-124f297f8aac"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lord Aufidius, were you how:\n",
            "Here lie in straight it Rome would strike it.\n",
            "There, Tybalt still Aufidius. This light our holy\n",
            "Is wandering into his foul more any tale.\n",
            "Romeo and all this? an I knew that hand this\n",
            "Frown, which shall I tell deseter at the business\n",
            "As I, by my rastory: if was an hour.\n",
            "O, we cannot truly bow this of you: marry, I spy\n",
            "I spake to steal.\n",
            "\n",
            "BENVOLIO:\n",
            "Not fellow, go with chexents\n",
            "Attend never les under wit's in habouts' fool.\n",
            "\n",
            "MERCUTIO:\n",
            "Was contract is dead? another somedy.\n",
            "\n",
            "CORIOLANUS:\n",
            "They love be done to the heart, the art glorion.\n",
            "\n",
            "Messenger:\n",
            "Thou hast wounds now, thy souly garled, and so I condem'd\n",
            "talk flest youth.\n",
            "\n",
            "HERMIONE:\n",
            "Let's the day comfort intell.\n",
            "\n",
            "First Walconspirator:\n",
            "Can do you the mother?\n",
            "\n",
            "BENVOLIO:\n",
            "What is there?\n",
            "\n",
            "Pray, pale:\n",
            "These eyesle, fear you comes to in the king;\n",
            "For I feel these it lies ones;\n",
            "He craft depose away come infire; and be it best,\n",
            "And by the greatest lords sight to her walls men\n",
            "To blush grassely. Come, good friar.\n",
            "O miserable: as you pare, good conference.\n",
            "Is it this easier is, but the nature\n",
            "Hath been success'd hath quarrel all the power\n",
            "At aftering out-cours of the charge\n",
            "And in the sunsiveral growing of him.\n",
            "So she marrives his gropp'd sights, with one there use\n",
            "For Withink of alter's and herself; and where they condemn'd,\n",
            "Suddeduct our nevolence the crouds lies\n",
            "His person slow and crate heart.' Then the warlike gaves\n",
            "And leisure with the mourn and dukedons,\n",
            "Set it from me so order in our English'd,\n",
            "Sit lies violent to imprise their pould:\n",
            "And then against them Gave pity in son privilegn,\n",
            "By it obscent may disguish it,\n",
            "As if you wedly dreamt to bolt them and not\n",
            "That hath cut ffor me shall still not be your self\n",
            "The fire while is one, and I may despay on you;\n",
            "My grave is smocured murder where I be put of my;\n",
            "And what lies gone to Romeo great and day\n",
            "That thou'st broken you have a trest of you allever.\n",
            "\n",
            "BRUTUS:\n",
            "Larry, old Barnardine.\n",
            "\n",
            "ROMEO:\n",
            "I have now spent thy souther; for I have wards\n",
            "O'er banish'd thee where I lived on.\n",
            "\n",
            "DUKE ROMEO:\n",
            "Peace, in true, and Richard, cousin; let thy daught,\n",
            "Did I, attire singers, you are; to doubt,\n",
            "I'll not them double before than thou.\n",
            "Comme, Call hill you tell his warliken\n",
            "To visita's life which keys, i' was a pastone.\n",
            "And virtue, spit there by yourselves: that old back\n",
            "Haught have doctain's ten.\n",
            "Him to hold them, prove a vessare.\n",
            "And you do myself a name;\n",
            "And with a strain smeet's of mercy\n",
            "Where surves and remembring from a through.\n",
            "Will you be? Come, thou, there well high sitting Estisture.\n",
            "Hold me of Richard of France, I'll stand with mine\n",
            "The flock of Ashield I returners and Certen Fifth;\n",
            "Here cold back since will make straik depute.\n",
            "Since I sates in a march behodve Kildred,\n",
            "Cousin of the attemple of inquestion,\n",
            "I see thee mercy can consider mate.\n",
            "\n",
            "ELBOW:\n",
            "By good but he that fitters, gelving nights,\n",
            "I have some without at your sight hid eye\n",
            "By reason is the night.\n",
            "\n",
            "HORTENSIO:\n",
            "You do it comes the country,\n",
            "God knowledge all and in voyalour\n",
            "By thus sits London for the faith Mistress instrument\n",
            "Shall do it grace to your honour. Come well you\n",
            "Has withguts that sens her us.\n",
            "\n",
            "BUTUS:\n",
            "Q meannest me, sir.\n",
            "\n",
            "Lieuth:\n",
            "You be a state, we will; dear to heavy:\n",
            "These are rassly in a grave, another stand, as it is not\n",
            "scared me from a man again.\n",
            "\n",
            "First Lord:\n",
            "Well, that out Hereford?\n",
            "\n",
            "CLAUDIO:\n",
            "Frier, the heaven said, told Norfolk as figulleng:\n",
            "\n",
            "LEONTES:\n",
            "Woe must he cither, Camillo; come, come more,\n",
            "If he could with bust hit seem royall.\n",
            "Well he thee be so shorest thy returning kingdom?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Harl thou abhors hither:\n",
            "That He islikeness his yond as unless slow\n",
            "Which it did visit no coster traitor. What\n",
            "'Tis the Tarpet, sister, master; of\n",
            "Clowless at esternal: cruptual I hide the ear:\n",
            "I have you but serve sad way her snate;\n",
            "For have you pursuage her sight's happly rubs,\n",
            "And therefore louds hers will look wot boon.\n",
            "Now shall I by my treacherous shall occuse me.\n",
            "Hords, wars go wont him! noble mad!\n",
            "Methinks you good my glorymege!\n",
            "\n",
            "KING HENRY VI:\n",
            "How long me shall the castle with the eagle in grace?\n",
            "\n",
            "RICHARD:\n",
            "My gracious lord's eyenestmoning king in brave?\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Thes somethinks of good poor, nor of words.\n",
            "\n",
            "GREY:\n",
            "So I protest? sure you?\n",
            "\n",
            "CLIFolopd:\n",
            "Conditain is that debt but who because warms\n",
            "That you shall have been: marry your royalty.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Talk your good liege, come me peace I'ld throw at.\n",
            "\n",
            "KING RICHARD IV:\n",
            "It just, I wish a robber; 'tis true.\n",
            "\n",
            "SThepherd:\n",
            "'Sir, ming is such one dreams on former:\n",
            "Marshal, you may come a melancholy hour.\n",
            "\n",
            "CLIFFORIZEL:\n",
            "The messent did cover: give me heaven and hear ta'en!\n",
            "I see you me: an you till you have dance\n",
            "In of the bad o' the matter; and 'tis mock dowly, I love.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "\n",
            "WDWISCK:\n",
            "You know me say?\n",
            "\n",
            "QUEEN HENRY BOLINGBROKE:\n",
            "We can be the king, as you must shall not confess\n",
            "A man but swear, true, battle, from on the grief.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Welcome, Catesby, as well down upon this glory.\n",
            "\n",
            "CLIFFORD:\n",
            "Then wouldst thou thus'st rag the flifty state\n",
            "I gave wicked thy wars and diggers prort,\n",
            "That well in arm touch'd morning to swear\n",
            "Than all so from the wafer: therefore to his spreed\n",
            "To take me remainted of that seat words forth his\n",
            "We should keps his dead\n",
            "And at advantage the cloth to his sword's enemy.\n",
            "Why doth not we'll have toes upon your ewards o' the daughter.\n",
            "\n",
            "FRIAR PERS:\n",
            "Nay, sister, let him be so able thought a soul\n",
            "And should have still be. For Hastisfact his mother:\n",
            "Edamined, you gean aband their state.\n",
            "\n",
            "CAMILLO:\n",
            "Now, do, widoward! I do it was his drums: it not sees,\n",
            "With imparted Clarence will he not, and I\n",
            "Upon this that he is queen is a man of him, that\n",
            "the hath a pissibda of presence in't.\n",
            "\n",
            "DIUS:\n",
            "No; brawl what, I have got thee, and be gone.\n",
            "\n",
            "PRUCINTIO:\n",
            "By ut suffice out--place!\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Sweet Romans, that bonds you can are his best.\n",
            "\n",
            "Provost:\n",
            "If you any grave shall all so were as you, as for a shame,\n",
            "Where you greetly stay behold bring past.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Why, then good good heart's up, you are lions.\n",
            "\n",
            "ISABELLA:\n",
            "Pray you, sir, I will not am publicly.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Heavens--\n",
            "\n",
            "LUCIO:\n",
            "I am some time with note. Hark, windred, sir.\n",
            "This Harry and him i' the Vower, and York.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Tell me what I implaintain, yinde quickly troop;\n",
            "To my babks and meant, I know so dead\n",
            "Is slain, for the Time are unkindled?\n",
            "\n",
            "QUEEN MARCHIO:\n",
            "\n",
            "QUET:\n",
            "Help Lord APNT:\n",
            "You besides, my son, sir.\n",
            "\n",
            "GLOUCESTER:\n",
            "Go hide thee mother, no; but ere I by dagger.\n",
            "Good morrow dothugh it, which you look some borrow:\n",
            "By yonder in else lord, in the sifew that crambing\n",
            "Whose your feels is examiedent know in to stand:\n",
            "Here comesly to your merites to be busing all.\n",
            "Petry, conscience, were at say they know to fight,\n",
            "That stay give true, nay thee: some grungb'd with your\n",
            "More this ward more can from sword your tune's blood.\n",
            "When more, adily, might: now strifless were you?\n",
            "Hence, Trave: who is you? Lander have done! As well, Greant man\n",
            "God jest. Good keeper to BretKisHister Brakench.\n",
            "What? hat not sees how right well as it me?\n",
            "\n",
            "LUCENTIO:\n",
            "A master to him, there's no manricion is delliver\n",
            "puttedical, not follow now\n",
            "en till to him down.\n",
            "I'll have me seen your lance to proud and all the tattes,\n",
            "now stretched hour's when from our soldiers,\n",
            "That for demn's mark-wittock an effect the read\n",
            "His true, playfellows, dropting the army.\n",
            "\n",
            "WARWICK:\n",
            "Ah, my hast thou slew not thy duke: if London,\n",
            "He cannot we were a king, I look'd thy business\n",
            "Laster betwing so for the Tower.\n",
            "\n",
            "KING RICHARD III:\n",
            "Well say thou tord o' fray that: but thou\n",
            "Find Huntsman: 'tis mock a horsest all covert\n",
            "The counting war, and the are ill high form\n",
            "The most repose of all men fall of piers,\n",
            "Why that we dint nature nave would wash him quirsel,\n",
            "And say 'God deny, mighty sins out of the fear\n",
            "A ptragon'd of beared pace to much hence,\n",
            "Yea, your beautient are stirright.\n",
            "Afor your sing four grace is a groanshing kings'\n",
            "The queen and is not well soil-bock,\n",
            "That wherew merry honour lies in those of Prencesx,\n",
            "Did live us with my hotes, but natureless ere in they,\n",
            "As obstoclary motherhood, imprint with boast,\n",
            "Be no doubt this husband, glave in folly.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "But will you be under beck\n",
            "Put upon a looking on a bedwell'd in this crown,\n",
            "I'll whip 't. Troy, master how thou boxes\n",
            "Before this grief, they are pleasure at lips by\n",
            "A graceful case small newly, and nameles on\n",
            "Heart--Elboard, whom I was the Towking-lazon\n",
            "With his mas-dragge, let such grace the yield.\n",
            "\n",
            "LADY CAPULET:\n",
            "He is as you, so gra-fatal.\n",
            "\n",
            "JULIET:\n",
            "O, lords, let Signior Horter--\n",
            "3fficer:\n",
            "Who, now priguing is claim in the rack;\n",
            "It hoves it may, holds her good morrow.\n",
            "\n",
            "JULIET:\n",
            "With cutsiffs, the barewelling of those are,\n",
            "Truth, to be sown'd to the London, and some bale,\n",
            "When he shall Richard Lartius. O, speak? poor treason!\n",
            "Talk, although I about to thy life dream'd, Ravenous master\n",
            "Where his is hearing and length, brother'st: why\n",
            "Thou didst pluck my steep's tongue at ogen.\n",
            "\n",
            "PRINCE:\n",
            "EDward but York home? Catesby: let the fear?\n",
            "\n",
            "CLARENCE:\n",
            "O he that hath best rats the effects,\n",
            "To blade these ambusadons what I becreedy at\n",
            "The head of the business of Vianna objects,\n",
            "Derick hated the prince to be your face into his son,\n",
            "When here be appair'd his town Adrian is;\n",
            "Receive Keys true, writing from thee away.\n",
            "\n",
            "ARCHMIDABEND:\n",
            "And shall, farewell our ask mine own lives,\n",
            "friend by thoughts meet no stale in that break.\n",
            "\n",
            "QUEEN:\n",
            "A place and is the droppli sympassess;\n",
            "He emblood excuse, he dies with king,\n",
            "To cut upon the follow of the hearest garless:\n",
            "The honour of Prince, that hath given that he since\n",
            "More warm as back with the gates, who to my thence\n",
            "rail'd to die.\n",
            "\n",
            "QUEEN:\n",
            "Do the villain bend two devil.\n",
            "\n",
            "GLOUCESTER:\n",
            "My Lord Angelo is the friend\n",
            "That Marshal espeech the prince to back of tear\n",
            "The wagoping.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "With it a captain a tongue so long-wits,\n",
            "Send give me husband with testiming committer.\n",
            "Withing the voirtuous parent friends to war\n",
            "her together! I must ever fall and time.\n",
            "\n",
            "CAMILLO:\n",
            "Where have she before him so? That's this happy, one.\n",
            "Courally hard Clifford\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('output.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "T047nPKDtVUZ",
        "outputId": "90c30591-2ed8-49fd-ecf9-0a73fe852d29"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6172181f-ff94-4bdf-a35f-23f1f4a0653c\", \"output.txt\", 10001)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Walkthrough of additional stuff"
      ],
      "metadata": {
        "id": "g4H_EGVLvjJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 nanoGPT repo\n",
        "\n"
      ],
      "metadata": {
        "id": "8QSIOc-nv63y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/karpathy/nanoGPT\n",
        "\n",
        "* most stuff in `model.py` similar:\n",
        "  * implements complex stuff in batches being added as the 4th dimension\n",
        "  * uses the GeLU non-linearity instead of ReLU to be able to load openAI checkpoints\n",
        "* most stuff in `train.py` is implemented in same way but more complex"
      ],
      "metadata": {
        "id": "_M0rwcaLx2Ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Training chatGPT ourselves\n",
        "\n",
        "<img src = \"https://raw.githubusercontent.com/Raahim58/Neural-networks/main/images/openAI.png\" height = 500 width = 500>"
      ],
      "metadata": {
        "id": "-MHsZU13wcBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://openai.com/blog/chatgpt/\n",
        "\n",
        "**1. Pre-Training and Fine-Tuning Stages of ChatGPT**:\n",
        "- **Training of ChatGPT** happens in two major stages: **pre-training** and **fine-tuning**.\n",
        "  - **Pre-training** involves training the model on a vast amount of internet data to get the decoder-only transformer to generate text.\n",
        "  - This pre-training phase is similar to what we've implemented ourselves, albeit at a much smaller scale.\n",
        "\n",
        "**2. Pre-training Example (Our Shakespeare Transformer)**:\n",
        "- The Shakespeare transformer we trained has **10 million parameters**.\n",
        "- The dataset we used for training consists of roughly **1 million characters**.\n",
        "- However, in larger AIs, models like GPT do not use a character model. Instead, they use **sub-word chunks**, with a vocabulary size of around **50,000 tokens**.\n",
        "  - In our case, the Shakespeare dataset equates to approximately **300,000 tokens**.\n",
        "\n",
        "**3. Large-Scale Pre-training in GPT**:\n",
        "- In comparison, large transformers, like GPT-3, have **175 billion parameters**.\n",
        "- GPT-3 was trained on a dataset of around **300 billion tokens**, much larger than our **300,000 tokens**.\n",
        "  - Today, modern models are being trained on datasets closer to **1 trillion tokens**.\n",
        "- Training such a large model requires a massive infrastructure, typically involving thousands of GPUs that communicate with one another.\n",
        "\n",
        "**4. Output of Pre-Training Stage**:\n",
        "- After pre-training, the model doesn't produce useful answers but instead \"babbles\" text.\n",
        "  - The output is not aligned to user questions and simply completes sequences, similar to completing a document.\n",
        "  - Without fine-tuning, the model might answer a question with another question or ignore it entirely.\n",
        "\n",
        "**5. Fine-Tuning Stage of ChatGPT**:\n",
        "- After pre-training, the model undergoes **fine-tuning** to transform it into a useful assistant.\n",
        "- **Fine-tuning** involves aligning the model to expect **question and answer** formats.\n",
        "  - OpenAI collects datasets where questions are on top and answers are below.\n",
        "  - These datasets contain **thousands of examples**, not as large as the pre-training data.\n",
        "  - The model is fine-tuned to complete answers after questions in a structured way.\n",
        "  - Large models are **sample-efficient**, making fine-tuning feasible even with smaller datasets.\n",
        "\n",
        "**6. Reinforcement Learning and Reward Models**:\n",
        "- **Step 1 of fine-tuning**: The model responds to queries, and different **raters rank responses** based on their quality.\n",
        "  - A **reward model** is trained to predict how desirable each response is.\n",
        "- **Step 2 of fine-tuning**: The reward model is used to fine-tune the main model further through **PPO (Policy Gradient Reinforcement Learning)**.\n",
        "  - This aligns the model to generate answers that score high rewards according to the reward model.\n",
        "\n",
        "**7. Final Output After Fine-Tuning**:\n",
        "- After fine-tuning, the model transitions from being a document completer to a **question-answering assistant**.\n",
        "- Most of the fine-tuning data is **internal to OpenAI** and not available publicly, making it challenging to replicate.\n",
        "\n",
        "**8. Summary**:\n",
        "- Today, we implemented a **decoder-only transformer**, which mimics the structure of models like **GPT**.\n",
        "- However, large-scale models like ChatGPT require both **pre-training** and **fine-tuning**, with infrastructure challenges and reinforcement learning involved in fine-tuning.\n"
      ],
      "metadata": {
        "id": "7LElvwrEx6S4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "TUaxmCcQwbU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Summary of the Session**:\n",
        "- We trained a **decoder-only transformer**, following the architecture from the famous **GPT paper** released in 2017.\n",
        "- The model was trained on the **Tiny Shakespeare dataset**, and we achieved sensible results.\n",
        "  - The complete training code is around **200 lines**.\n",
        "  - This codebase will be released soon, including **Git log commits** detailing the step-by-step process.\n",
        "\n",
        "**2. Resources**:\n",
        "- The model we trained is similar in architecture to **GPT-3**, but GPT-3 is **10,000 to 1 million times larger**, depending on how you measure it.\n",
        "\n",
        "**3. Focus of the Lecture**:\n",
        "- This lecture focused primarily on **language modeling** and the training of transformers.\n",
        "  - We did not delve into **fine-tuning** or advanced tasks such as **sentiment detection** or **task-specific alignment**.\n",
        "  - For more complex tasks beyond language modeling, **supervised fine-tuning** or more sophisticated approaches, such as **reinforcement learning with PPO**, are required.\n",
        "\n",
        "**4. Fine-Tuning and Advanced Techniques**:\n",
        "- To move beyond basic language modeling and make the model capable of specific tasks, **fine-tuning** is necessary.\n",
        "  - For example, **sentiment detection**, task-specific alignment, or the advanced **reward model training** seen in ChatGPT requires fine-tuning on a specialized dataset.\n",
        "  - **PPO (Policy Optimization)** can further align the model using reinforcement learning.\n",
        "\n",
        "**5. Conclusion and Next Steps**:\n",
        " - There's much more to explore in terms of **fine-tuning**, **reinforcement learning**, and **task-specific training**.\n",
        "\n"
      ],
      "metadata": {
        "id": "VCaitNBpy21K"
      }
    }
  ]
}